{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import sina.datastores.sql as sina_sql\n",
    "import sina.utils\n",
    "from sina.datastore import create_datastore\n",
    "from sina.visualization import Visualizer\n",
    "\n",
    "from trata import sampler as sm, composite_samples as cs, adaptive_sampler as ad\n",
    "from ibis import mcmc as mc, plots\n",
    "\n",
    "# %matplotlib notebook\n",
    "  \n",
    "# use this instead of %matplotlib notebook if using vscode, unless you install the matplotlib extension\n",
    "%matplotlib widget   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty Estimation via MCMC\n",
    "===============================\n",
    "In our bouncing ball example, we want to estimate the distribution of how many times the ball bounces off the ground until rest given a starting position, velocity, and gravity. \n",
    "\n",
    "An MCMC algorithm allows to simulate a probability distribution by constructing a Markov chain with the desired distribution as its stationary distribution. The MCMC algorithm iteratively updates the Markov chain based on the transition probability from one state to another state. Eventually, the chain attains the state of equilibrium when the joint probability distribution for the current state approaches the stationary distribution. The parameters that lead to the stationary distribution are considered as the model parameters learnt for the particular training image. \n",
    "\n",
    "Each state of a Markov chain is obtained by sampling a probability distribution. Among various sampling techniques, Metropolis algorithm and Gibbs sampler are two most well-known ones. See the appendix at the bottom for details. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling for Uncertainty Quantification\n",
    "\n",
    "The Gaussian process model requires specification of two kinds of inputs:\n",
    "\n",
    "1. $x$ = $(x_1,x_2,...,x_p)$ denotes inputs that are under the control of (or are observable by) the experimenter in both the field experiments and the simulator runs. In our bouncing ball example, there are $p=3$ inputs of this type:\n",
    "    - $x_1=R=1$, the radius of the ball\n",
    "    - $x_2=D=1$, the density of the ball\n",
    "    - $x_3=C=0.1$, the coefficient of drag\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. $\\theta$ = $(\\theta_1,\\theta_2,...,\\theta_q)$ denotes inputs to the simulator that are needed to estimate using the experimental data. These $\\theta$ could correspond to real physical quantities or could be parameters of the simulator code. In our bouncing ball example, there are $q=3$ inputs of this type: \n",
    "    - $\\theta_1=P=[0,100]$, the initial position of ball\n",
    "    - $\\theta_2=V=[-10,10]$, the initial velocity of the ball\n",
    "    - $\\theta_3=G=[1,10]$, the force of gravity acting on the ball\n",
    "\n",
    "\n",
    "$\\theta$ parameters of the bouncing ball model need to be appropriately sampled in order to build an accurate surrogate that honors the ground truth.\n",
    "\n",
    "### Generate Data\n",
    "To get startes we need to generate data by launching our bouncing ball studies by running the command:\n",
    "\n",
    "`maestro run ball_bounce_suite.yaml --pgen pgen.py`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialization\n",
    "\n",
    "database = 'output.sqlite'\n",
    "target_type = \"csv_rec\"\n",
    "datastore = create_datastore(database)\n",
    "recs = datastore.records\n",
    "vis = Visualizer(datastore)\n",
    "print(\"Sina is ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "groups = set(x[\"group_id\"][\"value\"] for x in recs.get_data([\"group_id\"]).values())\n",
    "\n",
    "print(\"So far we've run {} experiment groups each with 10 studies.\".format(len(groups)))\n",
    "print(\"We queried our database and found the following groups: {}\".format(groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated data, we need to load the inputs and output into our Markov chain Monte Carlo. \n",
    "Note that for each study in a group, gravity and initial position remain constant. It is the initial velocity of the ball that is changing between each study. \n",
    "\n",
    "### Trata\n",
    "\n",
    "Trata allows a user to create samples using various Bayesian sampling methods. Using Sina, we can easily access our \"observed\" input data and sample those values using the `DefaultValueSampler`. Had we not been able to, we could have sampled from the parameter ranges outlined above using various sampling methods provided by Trata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import statistics as sts\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "\n",
    "N = 10*len(groups)/2 # the number of samples should not exceed 10*groups\n",
    "gravity, x_pos, y_pos, z_pos, x_vel, y_vel, z_vel, bounces = [], [], [], [], [], [], [], []\n",
    "\n",
    "\n",
    "# instantiating the MCMC object\n",
    "\n",
    "vanilla_exp = mc.DefaultMCMC()\n",
    "\n",
    "\n",
    "# accessing and sampling the data for our surrogate model\n",
    "\n",
    "for group in groups:\n",
    "    id_pool = list(recs.find_with_data(group_id=group))\n",
    "    for rec_id in id_pool:\n",
    "        rec = recs.get(rec_id)\n",
    "\n",
    "        # data\n",
    "        data = rec.data_values\n",
    "        g = data[\"gravity\"]\n",
    "        gravity.append(g)\n",
    "        x = data[\"x_pos_initial\"]\n",
    "        x_pos.append(x)\n",
    "        y = data[\"y_pos_initial\"]\n",
    "        y_pos.append(y)\n",
    "        z = data[\"z_pos_initial\"]\n",
    "        z_pos.append(z)\n",
    "        vx = data[\"x_vel_initial\"]\n",
    "        x_vel.append(vx)\n",
    "        vy = data[\"y_vel_initial\"]\n",
    "        y_vel.append(vy)\n",
    "        vz = data[\"z_vel_initial\"]\n",
    "        z_vel.append(vz)\n",
    "        b = data[\"num_bounces\"]\n",
    "        bounces.append(b)\n",
    "\n",
    "        \n",
    "        # setting sample variables\n",
    "\n",
    "        samples = cs.Samples()\n",
    "        samples.set_continuous_variable('gravity', 1, g, 10)\n",
    "        samples.set_continuous_variable('x_pos_initial', 0, x, 100)\n",
    "        samples.set_continuous_variable('y_pos_initial', 0, y, 100)\n",
    "        samples.set_continuous_variable('z_pos_initial', 0, z, 100)\n",
    "        samples.set_continuous_variable('x_vel_initial', -10, vx, 10)\n",
    "        samples.set_continuous_variable('y_vel_initial', -10, vy, 10)\n",
    "        samples.set_continuous_variable('z_vel_initial', -10, vz, 10)\n",
    "        samples.set_continuous_variable('num_bounces', 0, b, 20)\n",
    "\n",
    "\n",
    "        # generating samples\n",
    "\n",
    "        samples.generate_samples(['gravity', 'x_pos_initial', 'y_pos_initial','z_pos_initial',\n",
    "                'x_vel_initial','y_vel_initial','z_vel_initial', 'num_bounces'], sm.DefaultValueSampler(), num_points = N)\n",
    "\n",
    "        points = samples.get_points(['gravity', 'x_pos_initial', 'y_pos_initial','z_pos_initial',\n",
    "                'x_vel_initial','y_vel_initial','z_vel_initial'])  \n",
    "\n",
    "\n",
    "        # the number of observed bounces will serve as our target training data while the target testing data will be randomly generated\n",
    "\n",
    "        train_num_bounces = samples.get_points(['num_bounces'])\n",
    "        test_num_bounces = randrange(20)\n",
    "\n",
    "\n",
    "        # we have opted to use a Guassian Process Regressor as our surrogate model for this problem\n",
    "\n",
    "        surrogate_model = GaussianProcessRegressor()\n",
    "        surrogate_model.fit(points, train_num_bounces)  # fitting the model using our parameter samples and the number of bounces\n",
    "\n",
    "\n",
    "        # adding the output training data and model to our MCMC\n",
    "\n",
    "        vanilla_exp.add_output('output', 'num_bounces', surrogate_model, test_num_bounces, sts.stdev(bounces) if len(bounces)>1 else 0, \n",
    "                ['gravity', 'x_pos_initial', 'y_pos_initial','z_pos_initial','x_vel_initial','y_vel_initial','z_vel_initial'])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating sigmas for each input\n",
    "\n",
    "sig_g = sts.stdev(gravity)\n",
    "sig_x = sts.stdev(x_pos)\n",
    "sig_y = sts.stdev(y_pos)\n",
    "sig_z = sts.stdev(z_pos)\n",
    "sig_vx = sts.stdev(x_vel)\n",
    "sig_vy = sts.stdev(y_vel)\n",
    "sig_vz = sts.stdev(z_vel)\n",
    "sig_b = sts.stdev(bounces)\n",
    "\n",
    "\n",
    "# adding the input data to our MCMC\n",
    "\n",
    "vanilla_exp.add_input('gravity', 1, 10, sig_g)\n",
    "vanilla_exp.add_input('x_pos_initial', 0, 100, sig_x)\n",
    "vanilla_exp.add_input('y_pos_initial', 0, 100, sig_y)\n",
    "vanilla_exp.add_input('z_pos_initial', 0, 100, sig_z)\n",
    "vanilla_exp.add_input('x_vel_initial', -10, 10, sig_vx)\n",
    "vanilla_exp.add_input('y_vel_initial', -10, 10, sig_vy)\n",
    "vanilla_exp.add_input('z_vel_initial', -10, 10, sig_vz)\n",
    "\n",
    "\n",
    "# running MCMC chain \n",
    "\n",
    "vanilla_exp.run_chain(total=10000, burn=10000, every=2, n_chains=16, prior_only=True)\n",
    "print(vanilla_exp.diagnostics_string())\n",
    "\n",
    "prior_chains = vanilla_exp.get_chains(flattened=True)\n",
    "prior_points = np.stack(prior_chains.values()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trace and autocorrelation plots for each input parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_trace(params):\n",
    "    for param in params:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\n",
    "        vanilla_exp.trace_plot(param, ax=axes[0])\n",
    "        vanilla_exp.autocorr_plot(param, ax=axes[1])\n",
    "        axes[0].title.set_text(\"{} trace plot\".format(param))\n",
    "        axes[0].set(xlabel=\"iteration\", ylabel=\"{} value\".format(param))\n",
    "        axes[1].title.set_text(\"{} autocorrelation plot\".format(param))\n",
    "        axes[1].set(xlabel=\"lag\", ylabel=\"ACF\")\n",
    "        fig.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auto_trace(['gravity','x_pos_initial','y_pos_initial','z_pos_initial','x_vel_initial','y_vel_initial','z_vel_initial'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before only the prior was considered, now the likelihood is calculated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vanilla_exp.run_chain(total=10000, burn=10000, every=30, n_chains=16, prior_only=False)\n",
    "print(vanilla_exp.diagnostics_string())\n",
    "\n",
    "post_chains = vanilla_exp.get_chains(flattened=True)\n",
    "post_points = np.stack(post_chains.values()).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_trace(['gravity','x_pos_initial','y_pos_initial','z_pos_initial','x_vel_initial','y_vel_initial','z_vel_initial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in post_chains.keys():\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.title.set_text(key)\n",
    "    plots.likelihood_plot(ax, prior_chains[key], post_points=post_chains[key])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "\n",
    "$\\hat{R}$ is a diagnostic that determines convergence (whether or not the chain has fully explored the whole distribution.) This value depends on the variance within chains and between chains. If this is too high it means that the chain has not been run long enough to fully converge to the target distribution.\n",
    "\n",
    "We need to adjust our `run_chain` parameters to allow for convergence. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "#### Metropolis algorithm \n",
    "\n",
    "Provides a mechanism to explore the entire configuration space by random walk. At each step, the algorithm performs a random modification to the current state to obtain a new state. The new state is either accepted or rejected with a probability computed based on energy change in the transition. The states generated by the algorithm form a Markov chain.\n",
    "\n",
    "|Metropolis Algorithm|\n",
    "|--------------------|\n",
    "|1: Randomize an input $g$.|\n",
    "|2: **repeat** |\n",
    "|3: &nbsp; Generate $g^*$ by performing a random trial move from $g$.|\n",
    "|4: &nbsp; Compute $Pr(g \\rightarrow g^*)=$ min $\\left\\{1,\\frac{Pr(g^*)}{Pr(g)}\\right\\}$|\n",
    "|5: &nbsp; if _random_ $(0,1]<Pr(g \\rightarrow g^*)$ then $g \\rightarrow g^*$.|\n",
    "|6: **until** equilibrium is attained.|\n",
    "\n",
    "\n",
    "#### Gibbs sampler \n",
    "A special case of the Metropolis algorithm, which generates new states by using univariate conditional probabilities. Because direct sampling from the complex joint distribution of all random variables is difficult, Gibbs sampler instead simulate random variables one by one from the univariate conditional distribution. A univariate conditional distribution involves only one random variable conditioned on the rest variables having fixed values, which is usually in a simple mathematical form.\n",
    "\n",
    "|Gibbs Sampling Algorithm|\n",
    "|------------------------|\n",
    "|1: Randomize an input $g$.|\n",
    "|2: **repeat** |\n",
    "|3: &nbsp; **for all** $i \\in R$ **do**|\n",
    "|4: &nbsp;&nbsp;&nbsp;&nbsp; Compute $Pr(g_i = q \\| g^i)$ for all $q \\in Q$.|\n",
    "|5: &nbsp;&nbsp;&nbsp;&nbsp; Assign $g_i$ to the value $q$ with the probability $Pr(g_i = q \\| g^i)$.|\n",
    "|6: &nbsp; **end for** |\n",
    "|7: **until** equilibrium is attained.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ball-bounce-demo",
   "language": "python",
   "name": "ball-bounce-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "f02797b5859f849067f9633f7cfc271f2096a19271761f3e16fd43533275a18d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
