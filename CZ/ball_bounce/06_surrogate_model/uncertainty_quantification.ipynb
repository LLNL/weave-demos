{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import sina.datastores.sql as sina_sql\n",
    "import sina.utils\n",
    "from sina.datastore import create_datastore\n",
    "from sina.visualization import Visualizer\n",
    "\n",
    "from trata import sampler as sm, composite_samples as cs, adaptive_sampler as ad\n",
    "from ibis import mcmc as mc, plots\n",
    "import themis\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if sys.argv[1] == '-f':  # Not sure why this returns '-f' if no arguments are passed?\n",
    "    spec_root = ''\n",
    "    # use %matplotlib widget instead of %matplotlib notebook if using vscode, unless you install the matplotlib extension\n",
    "    %matplotlib widget\n",
    "else:\n",
    "    spec_root = sys.argv[1]  # Modify path for Python script since this is running within its own step\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uncertainty Estimation via MCMC\n",
    "===============================\n",
    "In our bouncing ball example, we want to estimate the distribution of how many times the ball bounces off the ground until rest given a starting position, velocity, and gravity. \n",
    "\n",
    "An MCMC algorithm allows to simulate a probability distribution by constructing a Markov chain with the desired distribution as its stationary distribution. The MCMC algorithm iteratively updates the Markov chain based on the transition probability from one state to another state. Eventually, the chain attains the state of equilibrium when the joint probability distribution for the current state approaches the stationary distribution. The parameters that lead to the stationary distribution are considered as the model parameters learnt for the particular training image. \n",
    "\n",
    "Each state of a Markov chain is obtained by sampling a probability distribution. Among various sampling techniques, Metropolis algorithm and Gibbs sampler are two most well-known ones. See the appendix at the bottom for details. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling for Uncertainty Quantification\n",
    "\n",
    "The Gaussian process model requires specification of two kinds of inputs:\n",
    "\n",
    "1. $x$ = $(x_1,x_2,...,x_p)$ denotes inputs that are under the control of (or are observable by) the experimenter in both the field experiments and the simulator runs. In our bouncing ball example, there are $p=3$ inputs of this type:\n",
    "    - $x_1=R=1$, the radius of the ball\n",
    "    - $x_2=D=1$, the density of the ball\n",
    "    - $x_3=C=0.1$, the coefficient of drag\n",
    "    \n",
    "<br>\n",
    "\n",
    "2. $\\theta$ = $(\\theta_1,\\theta_2,...,\\theta_q)$ denotes inputs to the simulator that are needed to estimate using the experimental data. These $\\theta$ could correspond to real physical quantities or could be parameters of the simulator code. In our bouncing ball example, there are $q=3$ inputs of this type: \n",
    "    - $\\theta_1=P=[0,100]$, the initial position of ball\n",
    "    - $\\theta_2=V=[-10,10]$, the initial velocity of the ball\n",
    "    - $\\theta_3=G=[1,10]$, the force of gravity acting on the ball\n",
    "\n",
    "\n",
    "$\\theta$ parameters of the bouncing ball model need to be appropriately sampled in order to build an accurate surrogate that honors the ground truth.\n",
    "\n",
    "### Load Data\n",
    "To get started we need to load our simulation data by creating a Sina DataStore.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensembles Initialization\n",
    "database = os.path.join(spec_root, '../04_manage_data/data/ensembles_output.sqlite')\n",
    "target_type = \"csv_rec\"\n",
    "datastore = create_datastore(database)\n",
    "recs = datastore.records\n",
    "vis = Visualizer(datastore)\n",
    "print(\"Sina is ready!\")\n",
    "\n",
    "# Baseline Initialization\n",
    "database_baseline = os.path.join(spec_root, '../01_baseline_simulation/baseline/data/baseline_output.sqlite')\n",
    "datastore_baseline = create_datastore(database_baseline)\n",
    "recs_baseline = datastore_baseline.records\n",
    "vis_baseline = Visualizer(datastore_baseline)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_groups = set(x[\"group_id\"][\"value\"] for x in recs_baseline.get_data([\"group_id\"]).values())\n",
    "\n",
    "print(\"So far we've run {} experiment group(s) each with 10 studies in our baseline simualtion.\".format(len(base_groups)))\n",
    "print(\"We queried our database and found the following groups: {}\".format(base_groups))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to check the range of our target variable, the `num_bounces`, across our data groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_= vis_baseline.create_histogram(\"num_bounces\", interactive=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics as sts\n",
    "\n",
    "base_groups = set(x[\"group_id\"][\"value\"] for x in recs_baseline.get_data([\"group_id\"]).values())\n",
    "\n",
    "\n",
    "# storing the simulation data locally to calculate descriptive statistics\n",
    "num_bounces_test, gravity, x_pos, y_pos, z_pos, x_vel, y_vel, z_vel = [], [], [], [], [], [], [], []\n",
    "\n",
    "# accessing and sampling the data for our surrogate model\n",
    "for group in base_groups:\n",
    "    id_pool = list(recs_baseline.find_with_data(group_id=group))\n",
    "    for _, rec_id in enumerate(id_pool):\n",
    "        rec = recs_baseline.get(rec_id)\n",
    "        data = rec.data_values\n",
    "        num_bounces_test.append(data[\"num_bounces\"])\n",
    "        gravity.append(data[\"gravity\"])\n",
    "        x_pos.append(data[\"x_pos_initial\"])\n",
    "        y_pos.append(data[\"y_pos_initial\"])\n",
    "        z_pos.append(data[\"z_pos_initial\"])\n",
    "        x_vel.append(data[\"x_vel_initial\"])\n",
    "        y_vel.append(data[\"y_vel_initial\"])\n",
    "        z_vel.append(data[\"z_vel_initial\"])\n",
    "\n",
    "# calculating sigma for our output\n",
    "nb_std = sts.stdev(num_bounces_test)\n",
    "\n",
    "# calculating sigmas for each input\n",
    "sig_g = sts.stdev(gravity)\n",
    "sig_x = sts.stdev(x_pos)\n",
    "sig_y = sts.stdev(y_pos)\n",
    "sig_z = sts.stdev(z_pos)\n",
    "sig_vx = sts.stdev(x_vel)\n",
    "sig_vy = sts.stdev(y_vel)\n",
    "sig_vz = sts.stdev(z_vel)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've generated data, we need to load the inputs and output into our Markov chain Monte Carlo. \n",
    "\n",
    "**Note** that for each study in a group, gravity and initial position remain constant. It is the initial velocity of the ball that is changing between each study. \n",
    "\n",
    "### Trata\n",
    "\n",
    "The Trata package allows a user to create samples using various Bayesian sampling methods. Using Sina, we can easily access our \"observed\" input data and sample those values using the `DefaultValueSampler`. We are assuming we know the gravity acting on the ball and we can count the number of bounces each ball makes. These will be our observables, the former being a feature and the later being our target. \n",
    "\n",
    "We also have other features, or parameters, that affect the number of times the ball bounces, which include the position the ball is dropped and the initial velocity of the dropped ball. We will assume those are our unobserved features, or parameters, that introduce uncertainty. We will sample those parameters from the parameter ranges outlined above using `Latin Hypercube Sampling` (LHS) method provided by Trata."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Themis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "\n",
    "# number of samples from parameter space to be generated\n",
    "N = 1024/len(base_groups) \n",
    "\n",
    "# instantiating the MCMC object\n",
    "vanilla_exp = mc.DefaultMCMC()\n",
    "\n",
    "# accessing and sampling the data for our surrogate model\n",
    "# we need to allocate training data from our previously run simulations to train a machine learning surrogate model\n",
    "# we will be training our model on gravity and num_bounces from our simulation data as well as position and velocity samples from Trata\n",
    "for _, (g, b) in enumerate(zip(gravity, num_bounces_test)):\n",
    "        # setting sample parameter variables\n",
    "        samples = cs.Samples()\n",
    "        samples.set_continuous_variable('gravity', 0, g, 10)\n",
    "        samples.set_continuous_variable('x_pos_initial', 47.5, 49.0, 50.5)\n",
    "        samples.set_continuous_variable('y_pos_initial', 48.5, 50.0, 51.5)\n",
    "        samples.set_continuous_variable('z_pos_initial', 49.5, 51.0, 52.5)\n",
    "        samples.set_continuous_variable('x_vel_initial', 4.80, 5.25, 5.70)\n",
    "        samples.set_continuous_variable('y_vel_initial', 4.45, 4.9, 5.35)\n",
    "        samples.set_continuous_variable('z_vel_initial', 4.55, 5.0, 5.45)\n",
    "\n",
    "        # generating samples\n",
    "        samples.generate_samples(['x_pos_initial', 'y_pos_initial','z_pos_initial',\n",
    "                'x_vel_initial','y_vel_initial','z_vel_initial'], sm.LatinHyperCubeSampler(), num_points = N)  \n",
    "        samples.generate_samples(['gravity'], sm.DefaultValueSampler(), num_points = N) \n",
    "\n",
    "        runs = [themis.Run(sample, args='--input_deck input_deck_themis') for sample in samples]      \n",
    "        mgr = themis.Themis.create_overwrite('../03_simulation_ensembles/ball_bounce_themis',\n",
    "            runs=runs,\n",
    "            run_dir_names=os.path.join('../03_simulation_ensembles/data/themis_ensembles/ensemble/ens_' + str(_), '{run_id}'),\n",
    "            run_parse='../03_simulation_ensembles/input_deck_themis',\n",
    "            setup_dir=\"../03_simulation_ensembles/data/themis_ensembles/ensemble/setup/.ens_\" + str(_) + \"_setup\",\n",
    "            app_is_batch_script=False,\n",
    "        )\n",
    "        mgr.execute_local(blocking=True)\n",
    "        print(\"run{}: \".format(_+1), mgr.progress())  \n",
    "\n",
    "        # our target training data\n",
    "        train_num_bounces = mgr.as_dataframe(include_none=True)[[\"result\"]].to_numpy() \n",
    "        \n",
    "        # our sampled feature training data\n",
    "        train_features = samples.get_points(['gravity', 'x_pos_initial', 'y_pos_initial','z_pos_initial',\n",
    "                'x_vel_initial','y_vel_initial','z_vel_initial'], scaled=False)        \n",
    "        \n",
    "        # we have opted to use a Guassian Process Regressor as our surrogate model for this problem\n",
    "        surrogate_model = GaussianProcessRegressor()\n",
    "\n",
    "        # fitting the surrogate model using our parameter samples and the number of bounces simulated for those parameters\n",
    "        surrogate_model.fit(train_features, train_num_bounces)        \n",
    "        \n",
    "        # adding the target testing data and our fitted model as output to the MCMC for each observed num_bounces\n",
    "        vanilla_exp.add_output('output', 'num_bounces', surrogate_model, b, nb_std, \n",
    "                ['gravity', 'x_pos_initial', 'y_pos_initial','z_pos_initial','x_vel_initial','y_vel_initial','z_vel_initial'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Maestro and Merlin "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import pandas as pd\n",
    "\n",
    "groups = set(x[\"group_id\"][\"value\"] for x in recs.get_data([\"group_id\"]).values())\n",
    "\n",
    "# instantiating the MCMC object\n",
    "vanilla_exp = mc.DefaultMCMC()\n",
    "\n",
    "# accessing and sampling the data for our surrogate model\n",
    "# we need to allocate training data from our previously run simulations to train a machine learning surrogate model\n",
    "# we will be training our model on gravity and num_bounces from our simulation data as well as position and velocity samples from Trata\n",
    "\n",
    "num_bounces_train, gravity_train, x_pos_train, y_pos_train, z_pos_train, x_vel_train, y_vel_train, z_vel_train = [], [], [], [], [], [], [], []\n",
    "\n",
    "for group in groups:\n",
    "    id_pool = list(recs.find_with_data(group_id=group))\n",
    "    for _, rec_id in enumerate(id_pool):\n",
    "        rec = recs.get(rec_id)\n",
    "        data = rec.data_values\n",
    "        \n",
    "        # our feature training data\n",
    "        gravity_train.append(data[\"gravity\"])\n",
    "        x_pos_train.append(data[\"x_pos_initial\"])\n",
    "        y_pos_train.append(data[\"y_pos_initial\"])\n",
    "        z_pos_train.append(data[\"z_pos_initial\"])\n",
    "        x_vel_train.append(data[\"x_vel_initial\"])\n",
    "        y_vel_train.append(data[\"y_vel_initial\"])\n",
    "        z_vel_train.append(data[\"z_vel_initial\"])\n",
    "\n",
    "        # our target training data\n",
    "        num_bounces_train.append(data[\"num_bounces\"])\n",
    "\n",
    "\n",
    "# our sampled feature training data\n",
    "features_train = pd.DataFrame([gravity_train, x_pos_train, y_pos_train, z_pos_train, x_vel_train, y_vel_train, z_vel_train])\n",
    "features_train = features_train.transpose()\n",
    "num_bounces_train = pd.Series(num_bounces_train)\n",
    "\n",
    "# we have opted to use a Guassian Process Regressor as our surrogate model for this problem\n",
    "surrogate_model = GaussianProcessRegressor()\n",
    "\n",
    "# fitting the surrogate model using our parameter samples and the number of bounces simulated for those parameters\n",
    "surrogate_model.fit(features_train, num_bounces_train)        \n",
    "\n",
    "# adding the target testing data and our fitted model as output to the MCMC for each observed num_bounces\n",
    "for _ in num_bounces_test:\n",
    "    vanilla_exp.add_output('output', 'num_bounces', surrogate_model, _, nb_std, \n",
    "            ['gravity', 'x_pos_initial', 'y_pos_initial','z_pos_initial','x_vel_initial','y_vel_initial','z_vel_initial'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding Inputs to our MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding the input data to our MCMC\n",
    "vanilla_exp.add_input('gravity', min(gravity), max(gravity), sig_g)\n",
    "vanilla_exp.add_input('x_pos_initial', min(x_pos), max(x_pos), sig_x)\n",
    "vanilla_exp.add_input('y_pos_initial', min(y_pos), max(y_pos), sig_y)\n",
    "vanilla_exp.add_input('z_pos_initial', min(z_pos), max(z_pos), sig_z)\n",
    "vanilla_exp.add_input('x_vel_initial', min(x_vel), max(x_vel), sig_vx)\n",
    "vanilla_exp.add_input('y_vel_initial', min(y_vel), max(y_vel), sig_vy)\n",
    "vanilla_exp.add_input('z_vel_initial', min(z_vel), max(z_vel), sig_vz)\n",
    "\n",
    "\n",
    "# running MCMC chain \n",
    "vanilla_exp.run_chain(total=10000, burn=10000, every=2, n_chains=16, prior_only=True)\n",
    "print(vanilla_exp.diagnostics_string())\n",
    "\n",
    "prior_chains = vanilla_exp.get_chains(flattened=True)\n",
    "prior_points = np.stack(prior_chains.values()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Diagnostics\n",
    "\n",
    "$\\hat{R}$ is a diagnostic that determines convergence (whether or not the chain has fully explored the whole distribution.) This value depends on the variance within chains and between chains. If this is too high it means that the chain has not been run long enough to fully converge to the target distribution.\n",
    "\n",
    "#### Trace and Autocorrelation Plots for Each Input Parameter\n",
    "\n",
    "Trace plots of samples versus the simulation index can be very useful in assessing convergence. The trace tells you if the chain has not yet converged to its stationary distribution—that is, if it needs a longer burn-in period. A trace can also tell you whether the chain is mixing well. A chain might have reached stationarity if the distribution of points is not changing as the chain progresses. The aspects of stationarity that are most recognizable from a trace plot are a relatively constant mean and variance. A chain that mixes well traverses its posterior space rapidly, and it can jump from one remote region of the posterior to another in relatively few steps.\n",
    "\n",
    "From what we can observe from the trace plots produced, the results display a \"perfect\" trace plot. Note that the center of the chain appears to be around the midrange value of the parameter ranges, with very small fluctuations. This indicates that the chain *could* have reached the right distribution. The chain is mixing well; it is exploring the distribution by traversing to areas where its density is very low. You can conclude that the mixing is quite good here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def auto_trace(params):\n",
    "    for param in params:\n",
    "        fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(12, 7))\n",
    "        vanilla_exp.trace_plot(param, ax=axes[0])\n",
    "        vanilla_exp.autocorr_plot(param, ax=axes[1])\n",
    "        axes[0].title.set_text(\"{} trace plot\".format(param))\n",
    "        axes[0].set(xlabel=\"iteration\", ylabel=\"{} value\".format(param))\n",
    "        axes[1].title.set_text(\"{} autocorrelation plot\".format(param))\n",
    "        axes[1].set(xlabel=\"lag\", ylabel=\"ACF\")\n",
    "        fig.tight_layout()\n",
    "        fig.savefig(\"../06_surrogate_model/images/{}_trace_and_autocorrelation_plots.png\".format(param))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "auto_trace(['gravity','x_pos_initial','y_pos_initial','z_pos_initial','x_vel_initial','y_vel_initial','z_vel_initial'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before only the prior was considered, now the likelihood is calculated. We need to calculate the likelihood in order to generate likelihood plots discussed below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vanilla_exp.run_chain(total=10000, burn=10000, every=30, n_chains=16, prior_only=False)\n",
    "print(vanilla_exp.diagnostics_string())\n",
    "\n",
    "post_chains = vanilla_exp.get_chains(flattened=True)\n",
    "post_points = np.stack(post_chains.values()).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Likelihood Plots\n",
    "\n",
    "The likelihood is the probability that a particular outcome $x$ is observed when the true value of the parameter is $\\theta$, equivalent to the probability mass on $x$; it is *not* a probability density over the parameter $\\theta$.\n",
    "\n",
    "The probability plot is a graphical technique for assessing whether or not a data set follows a given distribution such as the normal distribution.\n",
    "\n",
    "The data are plotted against a theoretical distribution in such a way that the points should form approximately a straight line. Departures from this straight line indicate departures from the specified distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key in post_chains.keys():\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    ax.title.set_text(key)\n",
    "    plots.likelihood_plot(ax, prior_chains[key], post_points=post_chains[key])\n",
    "    fig.savefig(\"../06_surrogate_model/images/{}_likelihood_plot.png\".format(key))\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix\n",
    "\n",
    "#### Metropolis algorithm \n",
    "\n",
    "Provides a mechanism to explore the entire configuration space by random walk. At each step, the algorithm performs a random modification to the current state to obtain a new state. The new state is either accepted or rejected with a probability computed based on energy change in the transition. The states generated by the algorithm form a Markov chain.\n",
    "\n",
    "|Metropolis Algorithm|\n",
    "|--------------------|\n",
    "|1: Randomize an input $g$.|\n",
    "|2: **repeat** |\n",
    "|3: &nbsp; Generate $g^*$ by performing a random trial move from $g$.|\n",
    "|4: &nbsp; Compute $Pr(g \\rightarrow g^*)=$ min $\\left\\{1,\\frac{Pr(g^*)}{Pr(g)}\\right\\}$|\n",
    "|5: &nbsp; if _random_ $(0,1]<Pr(g \\rightarrow g^*)$ then $g \\rightarrow g^*$.|\n",
    "|6: **until** equilibrium is attained.|\n",
    "\n",
    "\n",
    "#### Gibbs sampler \n",
    "A special case of the Metropolis algorithm, which generates new states by using univariate conditional probabilities. Because direct sampling from the complex joint distribution of all random variables is difficult, Gibbs sampler instead simulate random variables one by one from the univariate conditional distribution. A univariate conditional distribution involves only one random variable conditioned on the rest variables having fixed values, which is usually in a simple mathematical form.\n",
    "\n",
    "|Gibbs Sampling Algorithm|\n",
    "|------------------------|\n",
    "|1: Randomize an input $g$.|\n",
    "|2: **repeat** |\n",
    "|3: &nbsp; **for all** $i \\in R$ **do**|\n",
    "|4: &nbsp;&nbsp;&nbsp;&nbsp; Compute $Pr(g_i = q \\| g^i)$ for all $q \\in Q$.|\n",
    "|5: &nbsp;&nbsp;&nbsp;&nbsp; Assign $g_i$ to the value $q$ with the probability $Pr(g_i = q \\| g^i)$.|\n",
    "|6: &nbsp; **end for** |\n",
    "|7: **until** equilibrium is attained.|"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Kosh Environment (1.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "f02797b5859f849067f9633f7cfc271f2096a19271761f3e16fd43533275a18d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
