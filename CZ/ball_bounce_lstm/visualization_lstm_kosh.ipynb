{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "**For more examples of what Kosh can do visit [GitHub Examples](https://github.com/LLNL/kosh/tree/stable/examples).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import kosh\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "if sys.argv[1] == '-f':  # Running as notebook\n",
    "    out_path = ''\n",
    "    %matplotlib notebook\n",
    "else:\n",
    "    out_path = sys.argv[1]  # Running as script\n",
    "\n",
    "# Ensembles Initialization\n",
    "database = os.path.join(out_path, 'ensembles_output.sqlite')\n",
    "print(database)\n",
    "datastore = kosh.connect(database)\n",
    "print(\"Kosh is ready!\")\n",
    "\n",
    "# Printing Attributes and Features\n",
    "test_rec = list(datastore.find())[1]\n",
    "print('Attributes:')\n",
    "print('\\t',test_rec.list_attributes())\n",
    "print('\\n')\n",
    "print('Features Sets:')\n",
    "print('\\t',test_rec.list_features())\n",
    "time=test_rec['physics_cycle_series/time'][:]\n",
    "image_path = os.path.join(out_path, 'lstm-ball-bounce/images')\n",
    "os.makedirs(image_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "\n",
    "Create a scaled Rank 3 Tensor for each of 60% Training, 20% Validation, and 20% Test Data.\n",
    "\n",
    "(# of Datasets, # of Time Steps per Dataset, # of Features per Time Step)\n",
    "\n",
    "**Cyclic Data**\n",
    "\n",
    "If the data is cyclical, the data can be split within each dataset since it has seen all the \"patterns\" of the cycle.\n",
    "\n",
    "**Acyclic Data**\n",
    "\n",
    "However, for acyclic data splitting it must be done at the dataset level instead of within the dataset since the model will have not seen the whole \"pattern\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Arrays\n",
    "X_train=np.array([])\n",
    "X_val=np.array([])\n",
    "X_test=np.array([])\n",
    "\n",
    "data_type = 'acyclic' # 'cyclic'\n",
    "\n",
    "if data_type == 'cyclic':\n",
    "\n",
    "    for i, dataset in enumerate(datastore.find(load_type='dictionary')): # Each record is now a dataset\n",
    "\n",
    "            print(f\"----------------------Dataset #{i}: ID: {dataset['id']}----------------------\")\n",
    "            if dataset['id']=='mean':\n",
    "                continue\n",
    "\n",
    "            x_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['x_pos']['value'][:]\n",
    "            y_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['y_pos']['value'][:]\n",
    "            z_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['z_pos']['value'][:]\n",
    "\n",
    "            # Current dataset matrix for features\n",
    "            X = pd.DataFrame([x_pos, y_pos, z_pos]).transpose()\n",
    "            X.columns=['x_pos','y_pos','z_pos']\n",
    "\n",
    "            # Splitting into train, validation, and test but since this is a time-series, it can't be shuffled\n",
    "            # Train: 60%, Validation: 20%, Test: 20%\n",
    "            X_train_temp, X_test_temp = train_test_split(X, test_size = 0.2, shuffle=False)\n",
    "            X_train_temp, X_val_temp = train_test_split(X_train_temp, test_size = 0.25, shuffle=False)\n",
    "\n",
    "            print('All Data:\\n',X,'\\n')\n",
    "            print('Training Data\\n', X_train_temp, '\\n')\n",
    "            print('Validation Data\\n', X_val_temp, '\\n')\n",
    "            print('Test Data\\n', X_test_temp, '\\n')\n",
    "\n",
    "            # Concatenating the tensor\n",
    "            if X_train.size==0:\n",
    "                X_train = np.array([X_train_temp.values])\n",
    "                X_val = np.array([X_val_temp.values ])\n",
    "                X_test = np.array([X_test_temp.values ])\n",
    "            else:\n",
    "                X_train = np.vstack((X_train,[X_train_temp.values]))\n",
    "                X_val =  np.vstack((X_val,[X_val_temp.values]))\n",
    "                X_test =  np.vstack((X_test,[X_test_temp.values]))            \n",
    "\n",
    "    # Plotting for cylical data\n",
    "    fig, ax = plt.subplots(nrows=3,sharex=True)\n",
    "    fig.suptitle('Example of Train, Val, and Test split')\n",
    "    time_train = time[:len(X_train_temp['x_pos'])]\n",
    "    time_val = time[len(X_train_temp['x_pos']):len(X_train_temp['x_pos'])+len(X_val_temp['x_pos'])]\n",
    "    time_test = time[len(X_train_temp['x_pos'])+len(X_val_temp['x_pos']):]\n",
    "\n",
    "    ax[0].plot(time,x_pos, label='Original')\n",
    "    ax[0].plot(time_train ,X_train_temp['x_pos'], label='Train')\n",
    "    ax[0].plot(time_val,X_val_temp['x_pos'], label='Validation')\n",
    "    ax[0].plot(time_test,X_test_temp['x_pos'], label='Test')\n",
    "    ax[0].legend(fontsize='xx-small')\n",
    "    ax[0].set_title('x_pos')\n",
    "\n",
    "    ax[1].plot(time,y_pos, label='Original')\n",
    "    ax[1].plot(time_train ,X_train_temp['y_pos'], label='Train')\n",
    "    ax[1].plot(time_val,X_val_temp['y_pos'], label='Validation')\n",
    "    ax[1].plot(time_test,X_test_temp['y_pos'], label='Test')\n",
    "    ax[1].legend(fontsize='xx-small')\n",
    "    ax[1].set_title('y_pos')\n",
    "\n",
    "    ax[2].plot(time,z_pos, label='Original')\n",
    "    ax[2].plot(time_train ,X_train_temp['z_pos'], label='Train')\n",
    "    ax[2].plot(time_val,X_val_temp['z_pos'], label='Validation')\n",
    "    ax[2].plot(time_test,X_test_temp['z_pos'], label='Test')\n",
    "    ax[2].legend(fontsize='xx-small')\n",
    "    ax[2].set_title('z_pos')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(image_path, 'cyclic_example_split.png'))\n",
    "\n",
    "elif data_type == 'acyclic':\n",
    "\n",
    "    num_datasets = len(list(datastore.find(load_type='dictionary'))) -1 # Subtract the 'mean' dataset from the other notebook\n",
    "    train_datasets = round(num_datasets*.6)\n",
    "    val_datasets = round(num_datasets*.2)\n",
    "    test_datasets = round(num_datasets*.2)    \n",
    "    print('Number of train, val, and test datasets:', train_datasets, val_datasets, test_datasets)\n",
    "\n",
    "    for i, dataset in enumerate(datastore.find(load_type='dictionary')): # Each record is now a dataset\n",
    "\n",
    "            print(f\"----------------------Dataset #{i}: ID: {dataset['id']}----------------------\")\n",
    "            if dataset['id']=='mean':\n",
    "                continue\n",
    "\n",
    "            x_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['x_pos']['value'][:]\n",
    "            y_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['y_pos']['value'][:]\n",
    "            z_pos = dataset['curve_sets']['physics_cycle_series']['dependent']['z_pos']['value'][:]\n",
    "\n",
    "            # Current dataset matrix for features\n",
    "            X = pd.DataFrame([x_pos, y_pos, z_pos]).transpose()\n",
    "            X.columns=['x_pos','y_pos','z_pos']\n",
    "\n",
    "            # Concatenating the tensor\n",
    "            if i<train_datasets:\n",
    "                if X_train.size==0:\n",
    "                    X_train = np.array([X.values])\n",
    "                else:\n",
    "                    X_train = np.vstack((X_train,[X.values])) \n",
    "            elif i<train_datasets+val_datasets:\n",
    "                if X_val.size==0:\n",
    "                    X_val = np.array([X.values ])\n",
    "                else:\n",
    "                    X_val =  np.vstack((X_val,[X.values]))\n",
    "            else:\n",
    "                if X_test.size==0:\n",
    "                    X_test = np.array([X.values ])\n",
    "                else:\n",
    "                    X_test =  np.vstack((X_test,[X.values]))   \n",
    "\n",
    "    # Plotting for acyclical data\n",
    "    fig, ax = plt.subplots(nrows=3,sharex=True)\n",
    "    fig.suptitle('Example of Train, Val, and Test split')\n",
    "\n",
    "    ax[0].plot(time,X_train[0,:,0], label='Train')\n",
    "    ax[0].plot(time,X_val[0,:,0], label='Validation')\n",
    "    ax[0].plot(time,X_test[0,:,0], label='Test')\n",
    "    ax[0].legend(fontsize='xx-small')\n",
    "    ax[0].set_title('x_pos')\n",
    "\n",
    "    ax[1].plot(time ,X_train[0,:,1], label='Train')\n",
    "    ax[1].plot(time,X_val[0,:,1], label='Validation')\n",
    "    ax[1].plot(time,X_test[0,:,1], label='Test')\n",
    "    ax[1].legend(fontsize='xx-small')\n",
    "    ax[1].set_title('y_pos')\n",
    "\n",
    "    ax[2].plot(time ,X_train[0,:,2], label='Train')\n",
    "    ax[2].plot(time,X_val[0,:,2], label='Validation')\n",
    "    ax[2].plot(time,X_test[0,:,2], label='Test')\n",
    "    ax[2].legend(fontsize='xx-small')\n",
    "    ax[2].set_title('z_pos')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(image_path, 'acyclic_example_split.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling the data\n",
    "Scaling the data so that all the features are around the same magnitude helps the model converge faster due to how the optimizers update the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_time_steps, num_features = X.shape\n",
    "print('Each whole dataset:',X.shape)\n",
    "print('\\tNumber of Time Steps in Each whole dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "##############\n",
    "# Train Data #\n",
    "##############\n",
    "num_datasets, num_time_steps, num_features = X_train.shape\n",
    "print('X_train:',X_train.shape)\n",
    "print('\\tNumber of Datasets:', num_datasets) \n",
    "print('\\tNumber of Time Steps per Dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Reshape each feature for all datasets into one long feature for scaling\n",
    "X_train = np.reshape(X_train, newshape=(-1, num_features))\n",
    "X_train = scaler.fit_transform(X_train) # Fit AND transform only for train data\n",
    "\n",
    "# Reshape each long feature back into their own dataset\n",
    "X_train_scaled = np.reshape(X_train, newshape=(num_datasets, num_time_steps, num_features))\n",
    "\n",
    "\n",
    "###################\n",
    "# Validation Data #\n",
    "###################\n",
    "num_datasets, num_time_steps, num_features = X_val.shape\n",
    "print('X_val:',X_val.shape)\n",
    "print('\\tNumber of Datasets:', num_datasets) \n",
    "print('\\tNumber of Time Steps per Dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Reshape each feature for all datasets into one long feature for scaling\n",
    "X_val = np.reshape(X_val, newshape=(-1, num_features))\n",
    "X_val = scaler.transform(X_val)  # Transform ONLY for validation data\n",
    "\n",
    "# Reshape each long feature back into their own dataset\n",
    "X_val_scaled = np.reshape(X_val, newshape=(num_datasets, num_time_steps, num_features))\n",
    "\n",
    "\n",
    "#############\n",
    "# Test Data #\n",
    "#############\n",
    "num_datasets, num_time_steps, num_features = X_test.shape\n",
    "print('X_test:',X_test.shape)\n",
    "print('\\tNumber of Datasets:', num_datasets) \n",
    "print('\\tNumber of Time Steps per Dataset:', num_time_steps) \n",
    "print('\\tNumber of Features per Time Step:', num_features) \n",
    "print('\\n')\n",
    "\n",
    "# Reshape each feature for all datasets into one long feature for scaling\n",
    "X_test = np.reshape(X_test, newshape=(-1, num_features))\n",
    "X_test = scaler.transform(X_test)  # Transform ONLY for test data\n",
    "\n",
    "# Reshape each long feature back into their own dataset\n",
    "X_test_scaled = np.reshape(X_test, newshape=(num_datasets, num_time_steps, num_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting Data into batches for LSTM\n",
    "\n",
    "Each dataset needs to be split into batches for LSTM to process. These batches are created such that the next time step is the y label. The size of the batch can be adjusted with `window`. Each batch increments a time step.\n",
    "\n",
    "The 0th batch of a single dataset with `window=3`:\n",
    "\n",
    "$$\n",
    "X_{train_{batch=0}} = \n",
    "\\begin{bmatrix}\n",
    "    [xpos_{t=0} & ypos_{t=0} & zpos_{t=0}] \\\\\n",
    "    [xpos_{t=1} & ypos_{t=1} & zpos_{t=1}] \\\\\n",
    "    [xpos_{t=2} & ypos_{t=2} & zpos_{t=2}]\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "y_{train_{batch=0}} = \n",
    "\\begin{bmatrix}\n",
    "    [xpos_{t=3} & ypos_{t=3} & zpos_{t=3}]\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "The 1st batch of a single dataset with `window=3`:\n",
    "\n",
    "$$\n",
    "X_{train_{batch=1}} = \n",
    "\\begin{bmatrix}\n",
    "    [xpos_{t=1} & ypos_{t=1} & zpos_{t=1}] \\\\\n",
    "    [xpos_{t=2} & ypos_{t=2} & zpos_{t=2}] \\\\\n",
    "    [xpos_{t=3} & ypos_{t=3} & zpos_{t=3}]\n",
    "\\end{bmatrix}\n",
    "\\\\\n",
    "y_{train_{batch=1}} = \n",
    "\\begin{bmatrix}\n",
    "    [xpos_{t=4} & ypos_{t=4} & zpos_{t=4}]\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "etc...\n",
    "\n",
    "However this will cause the tensor to become Rank 4 since we added the dimension of batches. We will need to reshape the X tensor to Rank 3 so that the LSTM can process it and since the y tensor only has 1 timestep of prediction, we will need to reshape it to Rank 2. If it was more than one timestep, we would turn it into a Rank 3 like X.\n",
    "\n",
    "X and y from Rank 4:\n",
    "\n",
    "(# of Datasets, # of Batches per Dataset, # of Time Steps per Batch, # of Features per Time Step)\n",
    "\n",
    "X to Rank 3:\n",
    "\n",
    "(# of Total Batches = Batches * Datasets, # of Time Steps per Batch, # of Features per Time Step)\n",
    "\n",
    "y to Rank 2:\n",
    "\n",
    "(# of Total Batches = Batches * Datasets, # of Features per Single Time Step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def lstm_split(X_train,window,X_val,X_test):\n",
    "\n",
    "    # Train data for lstm\n",
    "    num_datasets, num_time_steps, num_features = X_train.shape\n",
    "    total_batches_train = num_time_steps-window\n",
    "    X_train_lstm = np.array([])\n",
    "    y_train_lstm = np.array([])\n",
    "\n",
    "    for dataset in range(num_datasets):\n",
    "        print(f\"----------------------{dataset}----------------------\")\n",
    "        X_train_dataset = X_train[dataset]\n",
    "\n",
    "        X_train_lstm_temp =[]\n",
    "        y_train_lstm_temp = []\n",
    "\n",
    "        # Train data for lstm\n",
    "        for i in range(total_batches_train):\n",
    "            X_train_lstm_temp.append(X_train_dataset[i:window+i])\n",
    "            y_train_lstm_temp.append([X_train_dataset[window+i]])\n",
    "\n",
    "        X_train_lstm_temp = np.array(X_train_lstm_temp)\n",
    "        y_train_lstm_temp = np.array(y_train_lstm_temp)\n",
    "\n",
    "        \n",
    "        if X_train_lstm.size==0:\n",
    "            X_train_lstm = np.array([X_train_lstm_temp])\n",
    "            y_train_lstm = np.array([y_train_lstm_temp])\n",
    "            \n",
    "        else:\n",
    "            X_train_lstm = np.vstack((X_train_lstm,[X_train_lstm_temp]))\n",
    "            y_train_lstm = np.vstack((y_train_lstm,[y_train_lstm_temp]))\n",
    "\n",
    "    # Val data for lstm\n",
    "    num_datasets, num_time_steps, num_features = X_val.shape\n",
    "    total_batches_val = num_time_steps-window\n",
    "    X_val_lstm = np.array([])\n",
    "    y_val_lstm = np.array([])\n",
    "\n",
    "    for dataset in range(num_datasets):\n",
    "        print(f\"----------------------{dataset}----------------------\")\n",
    "        X_val_dataset = X_val[dataset]\n",
    "\n",
    "        X_val_lstm_temp =[]\n",
    "        y_val_lstm_temp = []\n",
    "\n",
    "        # Val data for lstm\n",
    "        for i in range(total_batches_val):\n",
    "            X_val_lstm_temp.append(X_val_dataset[i:window+i])\n",
    "            y_val_lstm_temp.append([X_val_dataset[window+i]])\n",
    "\n",
    "        X_val_lstm_temp = np.array(X_val_lstm_temp)\n",
    "        y_val_lstm_temp = np.array(y_val_lstm_temp)\n",
    "\n",
    "        \n",
    "        if X_val_lstm.size==0:\n",
    "            X_val_lstm = np.array([X_val_lstm_temp])\n",
    "            y_val_lstm =np.array([ y_val_lstm_temp])\n",
    "            \n",
    "        else:\n",
    "            X_val_lstm = np.vstack((X_val_lstm,[X_val_lstm_temp]))\n",
    "            y_val_lstm = np.vstack((y_val_lstm,[y_val_lstm_temp]))\n",
    "            \n",
    "\n",
    "    # Test data for lstm\n",
    "    num_datasets, num_time_steps, num_features = X_test.shape\n",
    "    total_batches_test = num_time_steps-window\n",
    "    X_test_lstm = np.array([])\n",
    "    y_test_lstm = np.array([])\n",
    "\n",
    "    for dataset in range(num_datasets):\n",
    "        print(f\"----------------------{dataset}----------------------\")\n",
    "        X_test_dataset = X_test[dataset]\n",
    "\n",
    "        X_test_lstm_temp =[]\n",
    "        y_test_lstm_temp = []\n",
    "\n",
    "        # Test data for lstm\n",
    "        for i in range(total_batches_test):\n",
    "            X_test_lstm_temp.append(X_test_dataset[i:window+i])\n",
    "            y_test_lstm_temp.append([X_test_dataset[window+i]])\n",
    "\n",
    "        X_test_lstm_temp = np.array(X_test_lstm_temp)\n",
    "        y_test_lstm_temp = np.array(y_test_lstm_temp)\n",
    "\n",
    "        \n",
    "        if X_test_lstm.size==0:\n",
    "            X_test_lstm = np.array([X_test_lstm_temp])\n",
    "            y_test_lstm =np.array([ y_test_lstm_temp])\n",
    "        else:\n",
    "            X_test_lstm = np.vstack((X_test_lstm,[X_test_lstm_temp]))\n",
    "            y_test_lstm = np.vstack((y_test_lstm,[y_test_lstm_temp]))\n",
    "            \n",
    "    ###################################\n",
    "    # Only one dataset for prediction #\n",
    "    ###################################\n",
    "    # Last dataset so we can compare to train, val, and test loop\n",
    "    X_train_lstm_for_predict = np.array([X_train_scaled[0]])\n",
    "    X_val_lstm_for_predict  = np.array([X_val_scaled[0]])\n",
    "    X_test_lstm_for_predict = np.array([X_test_scaled[0]])\n",
    "\n",
    "    return (X_train_lstm, y_train_lstm,\n",
    "            X_val_lstm, y_val_lstm,\n",
    "            X_test_lstm, y_test_lstm,\n",
    "            X_train_lstm_for_predict,\n",
    "            X_val_lstm_for_predict,\n",
    "            X_test_lstm_for_predict)\n",
    "\n",
    "window = 30  # This also determines how many data points are required to predict a new time series\n",
    "\n",
    "# All features\n",
    "(X_train_lstm, y_train_lstm,\n",
    " X_val_lstm, y_val_lstm,\n",
    " X_test_lstm, y_test_lstm,\n",
    " X_train_lstm_for_predict,\n",
    " X_val_lstm_for_predict,\n",
    " X_test_lstm_for_predict) = lstm_split(X_train_scaled, window, X_val_scaled, X_test_scaled)\n",
    "\n",
    "print('First 3 batches of a dataset')\n",
    "print('X_train LSTM: \\n',X_train_lstm[0,:3])\n",
    "print('y_train LSTM: \\n',y_train_lstm[0,:3])\n",
    "\n",
    "print('---------------Data grouped by datasets for LSTM------------------------------')\n",
    "print('\\n')\n",
    "print('X_train LSTM: ', X_train_lstm.shape)\n",
    "print('\\tNumber of Datasets:', X_train_lstm.shape[0]) \n",
    "print('\\tNumber of Batches per Dataset:', X_train_lstm.shape[1]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_train_lstm.shape[2]) \n",
    "print('\\tNumber of Features per Time Step:', X_train_lstm.shape[3]) \n",
    "print('\\n')\n",
    "print('y_train LSTM: ',y_train_lstm.shape)\n",
    "print('\\tNumber of Datasets:', y_train_lstm.shape[0]) \n",
    "print('\\tNumber of Batches per Dataset:', y_train_lstm.shape[1]) \n",
    "print('\\tNumber of Time Steps per Batch:', y_train_lstm.shape[2]) \n",
    "print('\\tNumber of Features per Time Step:', y_train_lstm.shape[3]) \n",
    "print('\\n')\n",
    "print('X_val LSTM: ',X_val_lstm.shape)\n",
    "print('\\tNumber of Datasets:', X_val_lstm.shape[0]) \n",
    "print('\\tNumber of Batches per Dataset:', X_val_lstm.shape[1]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_val_lstm.shape[2]) \n",
    "print('\\tNumber of Features per Time Step:', X_val_lstm.shape[3]) \n",
    "print('\\n')\n",
    "print('y_val LSTM: ',y_val_lstm.shape)\n",
    "print('\\tNumber of Datasets:', y_val_lstm.shape[0]) \n",
    "print('\\tNumber of Batches per Dataset:', y_val_lstm.shape[1]) \n",
    "print('\\tNumber of Time Steps per Batch:', y_val_lstm.shape[2]) \n",
    "print('\\tNumber of Features per Time Step:', y_val_lstm.shape[3]) \n",
    "print('\\n')\n",
    "print('X_test LSTM: ',X_test_lstm.shape)\n",
    "print('\\tNumber of Datasets:', X_test_lstm.shape[0]) \n",
    "print('\\tNumber of Batches per Dataset:', X_test_lstm.shape[1]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_test_lstm.shape[2]) \n",
    "print('\\tNumber of Features per Time Step:', X_test_lstm.shape[3]) \n",
    "print('\\n')\n",
    "print('y_test LSTM: ',y_test_lstm.shape)\n",
    "print('\\tNumber of Datasets:', y_test_lstm.shape[0]) \n",
    "print('\\tNumber of Batches per Dataset:', y_test_lstm.shape[1]) \n",
    "print('\\tNumber of Time Steps per Batch:', y_test_lstm.shape[2]) \n",
    "print('\\tNumber of Features per Time Step:', y_test_lstm.shape[3]) \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print('---------------Single dataset for LSTM Prediction------------------------------')\n",
    "print('\\n')\n",
    "print('X_train Whole:',X_train_lstm_for_predict.shape)\n",
    "print('\\tNumber of Batches per Single Dataset:', X_train_lstm_for_predict.shape[0]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_train_lstm_for_predict.shape[1]) \n",
    "print('\\tNumber of Features per Time Step:', X_train_lstm_for_predict.shape[2]) \n",
    "print('\\n')\n",
    "print('X_val Whole:',X_val_lstm_for_predict.shape)\n",
    "print('\\tNumber of Batches per Single Dataset:', X_val_lstm_for_predict.shape[0]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_val_lstm_for_predict.shape[1]) \n",
    "print('\\tNumber of Features per Time Step:', X_val_lstm_for_predict.shape[2]) \n",
    "print('\\n')\n",
    "print('X_test Whole:',X_test_lstm_for_predict.shape)\n",
    "print('\\tNumber of Batches per Single Dataset:', X_test_lstm_for_predict.shape[0]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_test_lstm_for_predict.shape[1]) \n",
    "print('\\tNumber of Features per Time Step:', X_test_lstm_for_predict.shape[2]) \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print('---------------Reshape data for LSTM which takes 3D data------------------------------')\n",
    "print('\\n')\n",
    "\n",
    "num_datasets, num_batches, num_time_steps, num_features = X_train_lstm.shape\n",
    "X_train_lstm = np.reshape(X_train_lstm, newshape=(num_batches*num_datasets, num_time_steps, num_features))\n",
    "num_datasets, num_batches, num_time_steps, num_features = y_train_lstm.shape\n",
    "y_train_lstm = np.reshape(y_train_lstm, newshape=(num_batches*num_datasets, num_features))\n",
    "\n",
    "print('X_train LSTM Reshaped: ', X_train_lstm.shape)\n",
    "print('\\tNumber of Total Batches = Datasets * Batches:', X_train_lstm.shape[0]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_train_lstm.shape[1]) \n",
    "print('\\tNumber of Features per Time Step:', X_train_lstm.shape[2]) \n",
    "print('\\n')\n",
    "print('y_train LSTM Reshaped: ',y_train_lstm.shape)\n",
    "print('\\tNumber of Total Batches = Datasets * Batches:', y_train_lstm.shape[0]) \n",
    "print('\\tNumber of Features per Single Time Step:', y_train_lstm.shape[1]) \n",
    "print('\\n')\n",
    "\n",
    "num_datasets, num_batches, num_time_steps, num_features_val = X_val_lstm.shape\n",
    "X_val_lstm = np.reshape(X_val_lstm, newshape=(num_batches*num_datasets, num_time_steps, num_features_val))\n",
    "num_datasets, num_batches, num_time_steps, num_features = y_val_lstm.shape\n",
    "y_val_lstm = np.reshape(y_val_lstm, newshape=(num_batches*num_datasets, num_features))\n",
    "\n",
    "print('X_val LSTM Reshaped: ', X_val_lstm.shape)\n",
    "print('\\tNumber of Total Batches = Datasets * Batches:', X_val_lstm.shape[0]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_val_lstm.shape[1]) \n",
    "print('\\tNumber of Features per Time Step:', X_val_lstm.shape[2]) \n",
    "print('\\n')\n",
    "print('y_val LSTM Reshaped: ',y_val_lstm.shape)\n",
    "print('\\tNumber of Total Batches = Datasets * Batches:', y_val_lstm.shape[0]) \n",
    "print('\\tNumber of Features per Single Time Step:', y_val_lstm.shape[1]) \n",
    "print('\\n')\n",
    "\n",
    "num_datasets, num_batches, num_time_steps, num_features_test = X_test_lstm.shape\n",
    "X_test_lstm = np.reshape(X_test_lstm, newshape=(num_batches*num_datasets, num_time_steps, num_features_test))\n",
    "num_datasets, num_batches, num_time_steps, num_features = y_test_lstm.shape\n",
    "y_test_lstm = np.reshape(y_test_lstm, newshape=(num_batches*num_datasets, num_features))\n",
    "\n",
    "print('X_test LSTM Reshaped: ', X_test_lstm.shape)\n",
    "print('\\tNumber of Total Batches = Datasets * Batches:', X_test_lstm.shape[0]) \n",
    "print('\\tNumber of Time Steps per Batch:', X_test_lstm.shape[1]) \n",
    "print('\\tNumber of Features per Time Step:', X_test_lstm.shape[2]) \n",
    "print('\\n')\n",
    "print('y_test LSTM Reshaped: ',y_test_lstm.shape)\n",
    "print('\\tNumber of Total Batches = Datasets * Batches:', y_test_lstm.shape[0]) \n",
    "print('\\tNumber of Features per Single Time Step:', y_test_lstm.shape[1]) \n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print('First 3 batches of a dataset Reshaped')\n",
    "print('X_train LSTM: \\n',X_train_lstm[:3])\n",
    "print('y_train LSTM: \\n',y_train_lstm[:3])\n",
    "print('\\n')\n",
    "print('\\n')\n",
    "\n",
    "print(\"Double check we are grabbing the same dataset for comparison later on\")\n",
    "print(scaler.inverse_transform(X_train_scaled[0])[:5])\n",
    "print(scaler.inverse_transform(X_train_lstm_for_predict[0])[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the Model\n",
    "Use tensorflow to create the model and add layers as needed. The `tf.keras.layers.LSTM()` should only have `return_sequences=True` if it is not the last LSTM layer. If it is the last LSTM layer `return_sequences=False` so that it only returns a single next time step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###################\n",
    "# Build the model #\n",
    "###################\n",
    "\n",
    "lstm_model = tf.keras.models.Sequential([\n",
    "    # return_sequences=False on last lstm layer so only one time step is returned\n",
    "    tf.keras.layers.LSTM(64,return_sequences=True), \n",
    "    tf.keras.layers.LSTM(32,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(16,return_sequences=True),\n",
    "    tf.keras.layers.LSTM(8,return_sequences=False),\n",
    "    tf.keras.layers.Dense(num_features)    # number of features\n",
    "])\n",
    "\n",
    "\n",
    "lstm_model.compile(loss='mse', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "###################\n",
    "# Train the model #\n",
    "###################\n",
    "# Early stopping just in case the model doesn't improve so we don't have to wait for all the epochs\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, \n",
    "                                                  restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = lstm_model.fit(X_train_lstm, y_train_lstm, \n",
    "                         epochs=100, # default epochs = 1\n",
    "                         validation_data = (X_val_lstm, y_val_lstm),\n",
    "                         callbacks=[early_stopping]\n",
    "                        )\n",
    "\n",
    "###########################\n",
    "# Plot the learning curve #\n",
    "###########################\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(history.history['loss'], label='Train')\n",
    "ax.plot(history.history['val_loss'], label='Val')\n",
    "ax.set_xlabel('epochs')\n",
    "ax.set_ylabel('MSE')\n",
    "ax.set_title('Train and Validation Loss')\n",
    "ax.legend()\n",
    "\n",
    "fig.savefig(os.path.join(image_path, 'learning_curve.png'))\n",
    "\n",
    "######################\n",
    "# Evaluate the model #\n",
    "######################\n",
    "print(lstm_model.summary())\n",
    "results = lstm_model.evaluate(X_train_lstm, y_train_lstm, batch_size=128)\n",
    "print(\"Train loss: \", results)\n",
    "results = lstm_model.evaluate(X_val_lstm, y_val_lstm, batch_size=128)\n",
    "print(\"Validation loss: \", results)\n",
    "results = lstm_model.evaluate(X_test_lstm, y_test_lstm, batch_size=128)\n",
    "print(\"Test loss: \", results)\n",
    "\n",
    "##################\n",
    "# Save the model #\n",
    "##################\n",
    "lstm_model_path = os.path.join(out_path, 'lstm-ball-bounce', 'my_lstm_model.keras')\n",
    "lstm_model.save(lstm_model_path)\n",
    "\n",
    "# You can also load it for other workflows\n",
    "# new_lstm_model = tf.keras.models.load_model(lstm_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Data\n",
    "\n",
    "Now that the model has been created, we can go ahead and predict our values. Since we only predict one step at a time, we need to keep predicting with a new window containing the newest prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Predict rest of series based on initial window:\n",
    "def prediction(X_predict, init_time, tot_time):\n",
    "    \n",
    "    X_predict_temp = np.array([X_predict[0][:window]])\n",
    "    \n",
    "    print(X_predict_temp)\n",
    "\n",
    "    X_whole = X_predict_temp\n",
    "    \n",
    "    n_loops = int(tot_time-init_time )\n",
    "    for i in range(n_loops):\n",
    "\n",
    "        print(f'Predicting time step: {init_time+i+1} of {tot_time}')\n",
    "        X_predict = lstm_model.predict(X_predict_temp)\n",
    "\n",
    "        # Concatenating to whole guess\n",
    "        X_whole=np.concatenate((X_whole,[X_predict]), axis=1)\n",
    "\n",
    "        # New prediction window with latest prediction\n",
    "        X_predict_temp = X_whole[:,-window:,:]\n",
    "\n",
    "    return X_whole\n",
    "\n",
    "if data_type == 'cyclic':\n",
    "\n",
    "    init_time_train = window\n",
    "    X_whole_train = prediction(X_train_lstm_for_predict, init_time_train, X.shape[0])\n",
    "    X_whole_train_unscaled = scaler.inverse_transform(X_whole_train[0])\n",
    "\n",
    "    init_time_val =  window+len(X_train_temp['x_pos'])\n",
    "    X_whole_val = prediction(X_val_lstm_for_predict,init_time_val, X.shape[0])\n",
    "    X_whole_val_unscaled = scaler.inverse_transform(X_whole_val[0])\n",
    "\n",
    "    init_time_test =  window+len(X_train_temp['x_pos'])+len(X_val_temp['x_pos'])\n",
    "    X_whole_test = prediction(X_test_lstm_for_predict,init_time_test, X.shape[0])\n",
    "    X_whole_test_unscaled = scaler.inverse_transform(X_whole_test[0])\n",
    "\n",
    "    #######################\n",
    "    # Plot the prediction #\n",
    "    #######################\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=3,sharex=True,figsize=(9,9))\n",
    "    fig.suptitle('Whole Prediction')\n",
    "    pos_train = scaler.inverse_transform(X_train_lstm_for_predict[0])\n",
    "    pos_val = scaler.inverse_transform(X_val_lstm_for_predict[0])\n",
    "    pos_test = scaler.inverse_transform(X_test_lstm_for_predict[0]) \n",
    "    \n",
    "    x_pos = list(pos_train[:,0]) +  list(pos_val[:,0]) +  list(pos_test[:,0])\n",
    "    ax[0].plot(time,x_pos, label='Original')\n",
    "    ax[0].plot(time[:window],X_whole_train_unscaled [:window,0], label='Train Initial Guess')\n",
    "    ax[0].plot(time[window-1:],X_whole_train_unscaled [window-1:,0], label='Train Whole Prediction')\n",
    "    ax[0].plot(time_val[:window],X_whole_val_unscaled [:window,0], label='Validation Initial Guess')\n",
    "    ax[0].plot(time[init_time_val-1:],X_whole_val_unscaled [window-1:,0], label='Validation Whole Prediction')\n",
    "    ax[0].plot(time_test[:window],X_whole_test_unscaled [:window,0], label='Test Initial Guess')\n",
    "    ax[0].plot(time[init_time_test-1:],X_whole_test_unscaled [window-1:,0], label='Test Whole Prediction')\n",
    "    ax[0].legend(fontsize='xx-small')\n",
    "    ax[0].set_title('x_pos')\n",
    "\n",
    "    y_pos = list(pos_train[:,1]) +  list(pos_val[:,1]) +  list(pos_test[:,1])\n",
    "    ax[1].plot(time,y_pos, label='Original')\n",
    "    ax[1].plot(time[:window],X_whole_train_unscaled [:window,1], label='Train Initial Guess')\n",
    "    ax[1].plot(time[window-1:],X_whole_train_unscaled [window-1:,1], label='Train Whole Prediction')\n",
    "    ax[1].plot(time_val[:window],X_whole_val_unscaled [:window,1], label='Validation Initial Guess')\n",
    "    ax[1].plot(time[init_time_val-1:],X_whole_val_unscaled [window-1:,1], label='Validation Whole Prediction')\n",
    "    ax[1].plot(time_test[:window],X_whole_test_unscaled [:window,1], label='Test Initial Guess')\n",
    "    ax[1].plot(time[init_time_test-1:],X_whole_test_unscaled [window-1:,1], label='Test Whole Prediction')\n",
    "    ax[1].legend(fontsize='xx-small')\n",
    "    ax[1].set_title('y_pos')\n",
    "\n",
    "    z_pos = list(pos_train[:,2]) +  list(pos_val[:,2]) +  list(pos_test[:,2])\n",
    "    ax[2].plot(time,z_pos, label='Original')\n",
    "    ax[2].plot(time[:window],X_whole_train_unscaled [:window,2], label='Train Initial Guess')\n",
    "    ax[2].plot(time[window-1:],X_whole_train_unscaled [window-1:,2], label='Train Whole Prediction')\n",
    "    ax[2].plot(time_val[:window],X_whole_val_unscaled [:window,2], label='Validation Initial Guess')\n",
    "    ax[2].plot(time[init_time_val-1:],X_whole_val_unscaled [window-1:,2], label='Validation Whole Prediction')\n",
    "    ax[2].plot(time_test[:window],X_whole_test_unscaled [:window,2], label='Test Initial Guess')\n",
    "    ax[2].plot(time[init_time_test-1:],X_whole_test_unscaled [window-1:,2], label='Test Whole Prediction')\n",
    "    ax[2].legend(fontsize='xx-small')\n",
    "    ax[2].set_title('z_pos')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(image_path, 'cyclic_whole_prediction.png'))\n",
    "    \n",
    "elif data_type == 'acyclic':\n",
    "\n",
    "    X_whole_train = prediction(X_train_lstm_for_predict, window, X.shape[0])\n",
    "    X_whole_train_unscaled = scaler.inverse_transform(X_whole_train[0])\n",
    "\n",
    "    X_whole_val = prediction(X_val_lstm_for_predict,window, X.shape[0])\n",
    "    X_whole_val_unscaled = scaler.inverse_transform(X_whole_val[0])\n",
    "\n",
    "    X_whole_test = prediction(X_test_lstm_for_predict,window, X.shape[0])\n",
    "    X_whole_test_unscaled = scaler.inverse_transform(X_whole_test[0])\n",
    "\n",
    "    #######################\n",
    "    # Plot the prediction #\n",
    "    #######################\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=3,ncols=3,sharex=True,figsize=(9,9))\n",
    "    fig.suptitle('Whole Prediction')\n",
    "\n",
    "    # x_pos\n",
    "    ax[0,0].plot(time,scaler.inverse_transform(X_train_lstm_for_predict[0])[:,0], label='Original')\n",
    "    ax[0,0].plot(time[:window],X_whole_train_unscaled [:window,0], label='Initial Guess')\n",
    "    ax[0,0].plot(time[window-1:],X_whole_train_unscaled [window-1:,0], label='Whole Prediction')\n",
    "    ax[0,0].legend(fontsize='xx-small')\n",
    "    ax[0,0].set_title('Train x_pos')\n",
    "\n",
    "    ax[0,1].plot(time,scaler.inverse_transform(X_val_lstm_for_predict[0])[:,0], label='Original')\n",
    "    ax[0,1].plot(time[:window],X_whole_val_unscaled [:window,0], label='Initial Guess')\n",
    "    ax[0,1].plot(time[window-1:],X_whole_val_unscaled [window-1:,0], label='Whole Prediction')\n",
    "    ax[0,1].legend(fontsize='xx-small')\n",
    "    ax[0,1].set_title('Validation x_pos')\n",
    "\n",
    "    ax[0,2].plot(time,scaler.inverse_transform(X_test_lstm_for_predict[0])[:,0], label='Original')\n",
    "    ax[0,2].plot(time[:window],X_whole_test_unscaled [:window,0], label='Initial Guess')\n",
    "    ax[0,2].plot(time[window-1:],X_whole_test_unscaled [window-1:,0], label='Whole Prediction')\n",
    "    ax[0,2].legend(fontsize='xx-small')\n",
    "    ax[0,2].set_title('Test x_pos')\n",
    "\n",
    "    # y_pos\n",
    "    ax[1,0].plot(time,scaler.inverse_transform(X_train_lstm_for_predict[0])[:,1], label='Original')\n",
    "    ax[1,0].plot(time[:window],X_whole_train_unscaled [:window,1], label='Initial Guess')\n",
    "    ax[1,0].plot(time[window-1:],X_whole_train_unscaled [window-1:,1], label='Whole Prediction')\n",
    "    ax[1,0].legend(fontsize='xx-small')\n",
    "    ax[1,0].set_title('Train y_pos')\n",
    "\n",
    "    ax[1,1].plot(time,scaler.inverse_transform(X_val_lstm_for_predict[0])[:,1], label='Original')\n",
    "    ax[1,1].plot(time[:window],X_whole_val_unscaled [:window,1], label='Initial Guess')\n",
    "    ax[1,1].plot(time[window-1:],X_whole_val_unscaled [window-1:,1], label='Whole Prediction')\n",
    "    ax[1,1].legend(fontsize='xx-small')\n",
    "    ax[1,1].set_title('Validation y_pos')\n",
    "\n",
    "\n",
    "    ax[1,2].plot(time,scaler.inverse_transform(X_test_lstm_for_predict[0])[:,1], label='Original')\n",
    "    ax[1,2].plot(time[:window],X_whole_test_unscaled [:window,1], label='Initial Guess')\n",
    "    ax[1,2].plot(time[window-1:],X_whole_test_unscaled [window-1:,1], label='Whole Prediction')\n",
    "    ax[1,2].legend(fontsize='xx-small')\n",
    "    ax[1,2].set_title('Test y_pos')\n",
    "\n",
    "    # y_pos\n",
    "    ax[2,0].plot(time,scaler.inverse_transform(X_train_lstm_for_predict[0])[:,2], label='Original')\n",
    "    ax[2,0].plot(time[:window],X_whole_train_unscaled [:window,2], label='Initial Guess')\n",
    "    ax[2,0].plot(time[window-1:],X_whole_train_unscaled [window-1:,2], label='Whole Prediction')\n",
    "    ax[2,0].legend(fontsize='xx-small')\n",
    "    ax[2,0].set_title('Train z_pos')\n",
    "\n",
    "    ax[2,1].plot(time,scaler.inverse_transform(X_val_lstm_for_predict[0])[:,2], label='Original')\n",
    "    ax[2,1].plot(time[:window],X_whole_val_unscaled [:window,2], label='Initial Guess')\n",
    "    ax[2,1].plot(time[window-1:],X_whole_val_unscaled [window-1:,2], label='Whole Prediction')\n",
    "    ax[2,1].legend(fontsize='xx-small')\n",
    "    ax[2,1].set_title('Validation z_pos')\n",
    "\n",
    "\n",
    "    ax[2,2].plot(time,scaler.inverse_transform(X_test_lstm_for_predict[0])[:,2], label='Original')\n",
    "    ax[2,2].plot(time[:window],X_whole_test_unscaled [:window,2], label='Initial Guess')\n",
    "    ax[2,2].plot(time[window-1:],X_whole_test_unscaled [window-1:,2], label='Whole Prediction')\n",
    "    ax[2,2].legend(fontsize='xx-small')\n",
    "    ax[2,2].set_title('Test z_pos')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(os.path.join(image_path, 'acyclic_whole_prediction.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cz-tutorials-demo",
   "language": "python",
   "name": "cz-tutorials-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
