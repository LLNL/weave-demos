{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "**For more examples of what Sina can do visit [GitHub Examples](https://github.com/LLNL/Sina/tree/master/examples).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import sina.datastores.sql as sina_sql\n",
    "import sina.utils\n",
    "from sina.datastore import create_datastore\n",
    "from sina.visualization import Visualizer\n",
    "from sina.model import Record, generate_record_from_json\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "if sys.argv[1] == '-f':  # Running as notebook\n",
    "    spec_root = ''\n",
    "    %matplotlib notebook\n",
    "else:\n",
    "    spec_root = sys.argv[1]  # Running as script\n",
    "\n",
    "# Ensembles Initialization\n",
    "database = os.path.join(spec_root, '../04_manage_data/data/ensembles_output.sqlite')\n",
    "target_type = \"csv_rec\"\n",
    "datastore = sina.connect(database)\n",
    "recs = datastore.records\n",
    "vis = Visualizer(datastore)\n",
    "print(\"Sina is ready!\")\n",
    "\n",
    "# Baseline Initialization\n",
    "database_baseline = os.path.join(spec_root, '../01_baseline_simulation/baseline/data/baseline_output.sqlite')\n",
    "datastore_baseline = sina.connect(database_baseline)\n",
    "recs_baseline = datastore_baseline.records\n",
    "group_id = '47bcda'\n",
    "val = recs_baseline.get(group_id + '_0')\n",
    "# Printing Data and Curvesets\n",
    "print('Data:')\n",
    "for data in val.data.keys():\n",
    "    print('\\t',data)\n",
    "print('\\n')\n",
    "print('Curve Sets:')\n",
    "for curve_set in val.curve_sets:\n",
    "    print('\\t',curve_set)\n",
    "    for cs_type in val.curve_sets[curve_set]:\n",
    "        print('\\t\\t',cs_type)\n",
    "        for curve in val.curve_sets[curve_set][cs_type]:\n",
    "            print('\\t\\t\\t',curve)\n",
    "\n",
    "cycle_set = val.get_curve_set(\"physics_cycle_series\")\n",
    "x_true = cycle_set.get_dependent('x_pos')['value']\n",
    "y_true = cycle_set.get_dependent('y_pos')['value']\n",
    "z_true = cycle_set.get_dependent('z_pos')['value']\n",
    "time_true = cycle_set.get_dependent('time')['value']\n",
    "\n",
    "# Numerical Resolution Initialization\n",
    "database_num_res = os.path.join(spec_root, '../01_baseline_simulation/num_res/data/num_res_output.sqlite')\n",
    "datastore_num_res = sina.connect(database_num_res)\n",
    "recs_num_res = datastore_num_res.records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Data to Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_rec = Record(id=\"mean\", type=\"summary\")\n",
    "recs.delete(\"mean\")\n",
    "\n",
    "x_temp = []\n",
    "y_temp = []\n",
    "z_temp = []\n",
    "\n",
    "x_mean = []\n",
    "y_mean = []\n",
    "z_mean = []\n",
    "x_std = []\n",
    "y_std = []\n",
    "z_std = []\n",
    "\n",
    "for i, t in enumerate(time_true):\n",
    "\n",
    "    for rec in recs.get_all():\n",
    "\n",
    "        cycle_set = rec.get_curve_set(\"physics_cycle_series\")\n",
    "        x_pred = cycle_set.get_dependent('x_pos')['value'][i]\n",
    "        y_pred = cycle_set.get_dependent('y_pos')['value'][i]\n",
    "        z_pred = cycle_set.get_dependent('z_pos')['value'][i]\n",
    "\n",
    "        x_temp.append(x_pred)\n",
    "        y_temp.append(y_pred)\n",
    "        z_temp.append(z_pred)\n",
    "\n",
    "    x_mean.append(statistics.mean(x_temp))\n",
    "    y_mean.append(statistics.mean(y_temp))\n",
    "    z_mean.append(statistics.mean(z_temp))\n",
    "    x_std.append(statistics.stdev(x_temp))\n",
    "    y_std.append(statistics.stdev(y_temp))\n",
    "    z_std.append(statistics.stdev(z_temp))\n",
    "\n",
    "    x_temp = []\n",
    "    y_temp = []\n",
    "    z_temp = []\n",
    "\n",
    "mean_set = mean_rec.add_curve_set(\"mean_data\")\n",
    "mean_set.add_independent('time', time_true)\n",
    "mean_set.add_dependent('x_pos_mean', x_mean)\n",
    "mean_set.add_dependent('y_pos_mean', y_mean)\n",
    "mean_set.add_dependent('z_pos_mean', z_mean)\n",
    "mean_set.add_dependent('x_pos_std', x_std)\n",
    "mean_set.add_dependent('y_pos_std', y_std)\n",
    "mean_set.add_dependent('z_pos_std', z_std)\n",
    "\n",
    "mean_set.add_dependent('x_pos_mean_plus_std', [x_mean[i] + x_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('y_pos_mean_plus_std', [y_mean[i] + y_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('z_pos_mean_plus_std', [z_mean[i] + z_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('x_pos_mean_minus_std', [x_mean[i] - x_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('y_pos_mean_minus_std', [y_mean[i] - y_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('z_pos_mean_minus_std', [z_mean[i] - z_std[i] for i in range(len(time_true))])\n",
    "\n",
    "mean_set.add_dependent('x_pos_mean_plus_2std', [x_mean[i] + 2 * x_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('y_pos_mean_plus_2std', [y_mean[i] + 2 * y_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('z_pos_mean_plus_2std', [z_mean[i] + 2 * z_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('x_pos_mean_minus_2std', [x_mean[i] - 2 * x_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('y_pos_mean_minus_2std', [y_mean[i] - 2 * y_std[i] for i in range(len(time_true))])\n",
    "mean_set.add_dependent('z_pos_mean_minus_2std', [z_mean[i] - 2 * z_std[i] for i in range(len(time_true))])\n",
    "\n",
    "recs.insert(mean_rec)  # need to update or else won't save!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "scalars = [\"x_pos_final\", \"y_pos_final\", \"z_pos_final\"]\n",
    "parameters = ['x_pos_initial', 'y_pos_initial', 'z_pos_initial', 'x_vel_initial', 'y_vel_initial', 'z_vel_initial']\n",
    "convergence = [1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024]\n",
    "n_bins = int(math.sqrt(convergence[-1]))\n",
    "os.makedirs(os.path.join(spec_root, \"../05_post-process_data/images/\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI transient data with uncertainty bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gathering Data\n",
    "\n",
    "mean = recs.get('mean')\n",
    "\n",
    "mean_set = mean.get_curve_set(\"mean_data\")\n",
    "time = mean_set.get_independent('time')['value']\n",
    "x_pos_mean_plus_2std = mean_set.get_dependent('x_pos_mean_plus_2std')['value']\n",
    "y_pos_mean_plus_2std = mean_set.get_dependent('y_pos_mean_plus_2std')['value']\n",
    "z_pos_mean_plus_2std = mean_set.get_dependent('z_pos_mean_plus_2std')['value']\n",
    "\n",
    "x_pos_mean_minus_2std = mean_set.get_dependent('x_pos_mean_minus_2std')['value']\n",
    "y_pos_mean_minus_2std = mean_set.get_dependent('y_pos_mean_minus_2std')['value']\n",
    "z_pos_mean_minus_2std = mean_set.get_dependent('z_pos_mean_minus_2std')['value']\n",
    "\n",
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "_ = vis.create_line_plot(fig=fig, ax=ax[0], x=\"time\", y=\"x_pos_mean\", title=\"{y_name}\", id_pool=['mean'])\n",
    "_ = vis.create_line_plot(fig=fig, ax=ax[1], x=\"time\", y=\"y_pos_mean\", title=\"{y_name}\", id_pool=['mean'])\n",
    "_ = vis.create_line_plot(fig=fig, ax=ax[2], x=\"time\", y=\"z_pos_mean\", title=\"{y_name}\", id_pool=['mean'])\n",
    "\n",
    "ax[0].fill_between(time, x_pos_mean_plus_2std, x_pos_mean_minus_2std, alpha=0.25)\n",
    "ax[1].fill_between(time, y_pos_mean_plus_2std, y_pos_mean_minus_2std, alpha=0.25)\n",
    "ax[2].fill_between(time, z_pos_mean_plus_2std, z_pos_mean_minus_2std, alpha=0.25)\n",
    "\n",
    "ax[0].plot(time_true, x_true)\n",
    "ax[1].plot(time_true, y_true)\n",
    "ax[2].plot(time_true, z_true)\n",
    "\n",
    "ax[0].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[1].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[2].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_u_input.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data violin and box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gathering Data\n",
    "\n",
    "final_data = recs.get_data(scalars)\n",
    "\n",
    "x_pos_final = [x[\"x_pos_final\"][\"value\"] for x in final_data.values()]\n",
    "y_pos_final = [x[\"y_pos_final\"][\"value\"] for x in final_data.values()]\n",
    "z_pos_final = [x[\"z_pos_final\"][\"value\"] for x in final_data.values()]\n",
    "\n",
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].violinplot(x_pos_final)\n",
    "ax[0].boxplot(x_pos_final)\n",
    "ax[1].violinplot(y_pos_final)\n",
    "ax[1].boxplot(y_pos_final)\n",
    "ax[2].violinplot(z_pos_final)\n",
    "ax[2].boxplot(z_pos_final)\n",
    "\n",
    "ax[0].set_title(\"x_pos_final\")\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[0].set_xticklabels([\"x_pos_final\"])\n",
    "\n",
    "ax[1].set_title(\"y_pos_final\")\n",
    "ax[1].set_xticklabels([\"y_pos_final\"])\n",
    "\n",
    "ax[2].set_title(\"z_pos_final\")\n",
    "ax[2].set_xticklabels([\"z_pos_final\"])\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_violin_box.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data violin and box convergence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scalar in scalars:\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 7))\n",
    "\n",
    "    for i, runs in enumerate(convergence):\n",
    "\n",
    "        convergence_ids = [group_id + \"_\" + str(x + 1) for x in range(runs)]  # We can do this because IDs have the run number\n",
    "\n",
    "        if runs == convergence[-1]:  # Will error if all ids are present\n",
    "            final_data = recs.get_data(scalars)\n",
    "        else:\n",
    "            final_data = recs.get_data(scalars, id_list=convergence_ids)\n",
    "\n",
    "        scalar_values = [x[scalar][\"value\"] for x in final_data.values()]\n",
    "\n",
    "        ax.violinplot(scalar_values, positions=[i])\n",
    "        ax.boxplot(scalar_values, positions=[i])\n",
    "\n",
    "    ax.set_title(scalar)\n",
    "    ax.set_xlabel(\"Simulations\")\n",
    "    ax.set_ylabel(\"Position\")\n",
    "    ax.set_xticklabels(convergence, rotation=45)\n",
    "\n",
    "    fig.savefig(os.path.join(spec_root, f\"../05_post-process_data/images/QoIs_{scalar}_violin_box_convergence.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data PDF and CDF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "ax1 = []\n",
    "ax1.append(ax[0].twinx())\n",
    "ax1.append(ax[1].twinx())\n",
    "ax1.append(ax[2].twinx())\n",
    "\n",
    "# x\n",
    "ax[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, label='PDF')\n",
    "ax1[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, color=colors[1], label='CDF')\n",
    "\n",
    "ax[0].set_title(\"x_pos_final\")\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "lines, labels = ax[0].get_legend_handles_labels()\n",
    "lines2, labels2 = ax1[0].get_legend_handles_labels()\n",
    "ax[0].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# y\n",
    "ax[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True)\n",
    "ax1[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, color=colors[1])\n",
    "\n",
    "ax[1].set_title(\"y_pos_final\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[1].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# z\n",
    "ax[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True)\n",
    "ax1[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, color=colors[1])\n",
    "\n",
    "ax[2].set_title(\"z_pos_final\")\n",
    "ax[2].set_xlabel(\"Position\")\n",
    "ax[2].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_pdf_cdf.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data PDF and CDF convergence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "fig1, ax1 = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "for i, runs in enumerate(convergence):  # Depending on number of runs, can obscure the rest\n",
    "\n",
    "    convergence_ids = [group_id + \"_\" + str(x + 1) for x in range(runs)]  # We can do this because IDs have the run number\n",
    "\n",
    "    if runs == convergence[-1]:  # Will error if all ids are present\n",
    "        final_data = recs.get_data(scalars)\n",
    "    else:\n",
    "        final_data = recs.get_data(scalars, id_list=convergence_ids)\n",
    "\n",
    "    x_pos_final = [x[\"x_pos_final\"][\"value\"] for x in final_data.values()]\n",
    "    y_pos_final = [x[\"y_pos_final\"][\"value\"] for x in final_data.values()]\n",
    "    z_pos_final = [x[\"z_pos_final\"][\"value\"] for x in final_data.values()]\n",
    "\n",
    "    ax[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, label=runs)\n",
    "    ax[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True, label=runs)\n",
    "    ax[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True, label=runs)\n",
    "\n",
    "    ax1[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, label=runs)\n",
    "    ax1[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, label=runs)\n",
    "    ax1[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, label=runs)\n",
    "\n",
    "# x\n",
    "ax[0].set_title(\"x_pos_final pdf\")\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "ax[0].legend()\n",
    "\n",
    "# y\n",
    "ax[1].set_title(\"y_pos_final pdf\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[1].legend()\n",
    "\n",
    "# z\n",
    "ax[2].set_title(\"z_pos_final pdf\")\n",
    "ax[2].set_xlabel(\"Position\")\n",
    "ax[2].legend()\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_pdf_convergence.png\"))\n",
    "\n",
    "# x\n",
    "ax1[0].set_title(\"x_pos_final cdf\")\n",
    "ax1[0].set_xlabel(\"Position\")\n",
    "ax1[0].legend()\n",
    "\n",
    "# y\n",
    "ax1[1].set_title(\"y_pos_final cdf\")\n",
    "ax1[1].set_xlabel(\"Position\")\n",
    "ax1[1].legend()\n",
    "\n",
    "# z\n",
    "ax1[2].set_title(\"z_pos_final cdf\")\n",
    "ax1[2].set_xlabel(\"Position\")\n",
    "ax1[2].legend()\n",
    "\n",
    "fig1.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_cdf_convergence.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data parameter correlation scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_plts = len(parameters)\n",
    "rows_cols = math.ceil(math.sqrt(num_plts))\n",
    "\n",
    "all_scalars = scalars + parameters\n",
    "\n",
    "corrcoefs = {}  # Correlation Coefficient Dictionary that will be used in the next two cells\n",
    "\n",
    "for i, runs in enumerate(convergence):\n",
    "\n",
    "    convergence_ids = [group_id + \"_\" + str(x + 1) for x in range(runs)]  # We can do this because IDs have the run number\n",
    "\n",
    "    if runs == convergence[-1]:  # will error if all ids are present\n",
    "        final_data = recs.get_data(all_scalars)\n",
    "    else:\n",
    "        final_data = recs.get_data(all_scalars, id_list=convergence_ids)\n",
    "\n",
    "    corrcoefs[runs] = {}\n",
    "\n",
    "    for scalar in scalars:\n",
    "\n",
    "        if runs == convergence[-1]:  # Just plot the last set of simulations\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=rows_cols, ncols=rows_cols, figsize=(rows_cols * 5, rows_cols * 5))\n",
    "\n",
    "            fig.suptitle(scalar)\n",
    "\n",
    "            i = 0\n",
    "            j = 0\n",
    "            ax[j, i].set_ylabel(scalar)\n",
    "\n",
    "        scalar_values = [x[scalar][\"value\"] for x in final_data.values()]\n",
    "\n",
    "        corrcoefs[runs][scalar] = {}\n",
    "\n",
    "        for parameter in parameters:\n",
    "\n",
    "            parameter_values = [x[parameter][\"value\"] for x in final_data.values()]\n",
    "\n",
    "            r = np.corrcoef(parameter_values, scalar_values)[0, 1]\n",
    "            corrcoefs[runs][scalar][parameter] = r\n",
    "\n",
    "            if runs == convergence[-1]:  # Just plot the last set of simulations\n",
    "\n",
    "                m, b = np.polyfit(parameter_values, scalar_values, 1)\n",
    "                print(f\"m: {m}, r: {r}\")\n",
    "                x = np.linspace(min(parameter_values), max(parameter_values))\n",
    "                y = m * x + b\n",
    "\n",
    "#                 slope, intercept, r, p, se = stats.linregress(x, y)\n",
    "\n",
    "                ax[j, i].scatter(parameter_values, scalar_values)\n",
    "                ax[j, i].plot(x, y, color=colors[1], linewidth=2.0)\n",
    "                ax[j, i].set_title(f\"{parameter} r={round(r, 2)}\")\n",
    "\n",
    "                if i == rows_cols - 1:  # Cycling through subplots\n",
    "\n",
    "                    i = 0\n",
    "                    j += 1\n",
    "                    ax[j, i].set_ylabel(scalar)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "        if runs == convergence[-1]:  # Just plot the last set of simulations\n",
    "            fig.savefig(os.path.join(spec_root, f\"../05_post-process_data/images/QoIs_{scalar}_correlation.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data parameter correlation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "cc_matrix = np.zeros((len(parameters), len(scalars)))\n",
    "\n",
    "for scalar in scalars:\n",
    "\n",
    "    for parameter in parameters:\n",
    "\n",
    "        cc_matrix[i, j] = corrcoefs[convergence[-1]][scalar][parameter]\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    j += 1\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "im = ax.imshow(cc_matrix)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set_title(\"Correlation Heatmap\")\n",
    "\n",
    "ax.set_xlabel(\"QoI\")\n",
    "ax.set_xticks(np.arange(len(scalars)))\n",
    "ax.set_xticklabels(scalars, rotation=90, ha='center', minor=False)\n",
    "\n",
    "ax.set_ylabel(\"Parameter\")\n",
    "ax.set_yticks(np.arange(len(parameters)))\n",
    "ax.set_yticklabels(parameters, minor=False)\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, '../05_post-process_data/images/QoIs_correlation_heatmap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data parameter correlation convergence heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "cc_matrix = np.zeros((len(parameters), len(convergence)))\n",
    "\n",
    "for scalar in scalars:\n",
    "\n",
    "    for runs in convergence:\n",
    "\n",
    "        for parameter in parameters:\n",
    "\n",
    "            cc_matrix[i, j] = corrcoefs[runs][scalar][parameter]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        j += 1\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "    im = ax.imshow(cc_matrix)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Correlation Heatmap Convergence {scalar}\")\n",
    "\n",
    "    ax.set_xlabel(\"Simulations\")\n",
    "    ax.set_xticks(np.arange(len(convergence)))\n",
    "    ax.set_xticklabels(convergence, rotation=90, ha='center', minor=False)\n",
    "\n",
    "    ax.set_ylabel(\"Parameter\")\n",
    "    ax.set_yticks(np.arange(len(parameters)))\n",
    "    ax.set_yticklabels(parameters, minor=False)\n",
    "\n",
    "    fig.savefig(os.path.join(spec_root, f'../05_post-process_data/images/QoIs_{scalar}_correlation_heatmap_convergence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding up the uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding common timestep data to Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Resolution\n",
    "# Be sure to run 01_baseline_simulation/num_res/visualization_num_res.ipynb to acquire necessary data\n",
    "mean_num_res = recs_num_res.get('mean')\n",
    "mean_set_num_res = mean_num_res.get_curve_set(\"mean_data\")\n",
    "time_num_res = mean_set_num_res.get_independent('time_common')['value']\n",
    "\n",
    "# Ensembles\n",
    "mean = recs.get('mean')\n",
    "mean_set = mean.get_curve_set(\"mean_data\")\n",
    "time = mean_set.get_independent('time')['value']\n",
    "x_pos_mean = mean_set.get_dependent('x_pos_mean')['value']\n",
    "y_pos_mean = mean_set.get_dependent('y_pos_mean')['value']\n",
    "z_pos_mean = mean_set.get_dependent('z_pos_mean')['value']\n",
    "x_pos_std = mean_set.get_dependent('x_pos_std')['value']\n",
    "y_pos_std = mean_set.get_dependent('y_pos_std')['value']\n",
    "z_pos_std = mean_set.get_dependent('z_pos_std')['value']\n",
    "\n",
    "x_pos_mean_common = []\n",
    "y_pos_mean_common = []\n",
    "z_pos_mean_common = []\n",
    "x_pos_std_common = []\n",
    "y_pos_std_common = []\n",
    "z_pos_std_common = []\n",
    "time_common = []\n",
    "\n",
    "for i, t in enumerate(time):\n",
    "\n",
    "    for t2 in time_num_res:\n",
    "\n",
    "        if t == t2:\n",
    "\n",
    "            x_pos_mean_common.append(x_pos_mean[i])\n",
    "            y_pos_mean_common.append(y_pos_mean[i])\n",
    "            z_pos_mean_common.append(z_pos_mean[i])\n",
    "            x_pos_std_common.append(x_pos_std[i])\n",
    "            y_pos_std_common.append(y_pos_std[i])\n",
    "            z_pos_std_common.append(z_pos_std[i])\n",
    "            time_common.append(time[i])\n",
    "\n",
    "common_set = mean.add_curve_set(\"common_data\")\n",
    "common_set.add_independent('time_common', time_common)\n",
    "common_set.add_dependent('x_pos_mean_common', x_pos_mean_common)\n",
    "common_set.add_dependent('y_pos_mean_common', y_pos_mean_common)\n",
    "common_set.add_dependent('z_pos_mean_common', z_pos_mean_common)\n",
    "common_set.add_dependent('x_pos_std_common', x_pos_std_common)\n",
    "common_set.add_dependent('y_pos_std_common', y_pos_std_common)\n",
    "common_set.add_dependent('z_pos_std_common', z_pos_std_common)\n",
    "\n",
    "recs.update(mean)  # need to update or else won't save!!!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Validation Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numerical Resolution: Numerical Uncertainty (u_num)\n",
    "# Be sure to run 01_baseline_simulation/num_res/visualization_num_res.ipynb to acquire necessary data\n",
    "x_pos_std_num_res = mean_set_num_res.get_dependent('x_pos_std')['value']\n",
    "y_pos_std_num_res = mean_set_num_res.get_dependent('y_pos_std')['value']\n",
    "z_pos_std_num_res = mean_set_num_res.get_dependent('z_pos_std')['value']\n",
    "\n",
    "# Ensembles: Input Uncertainty (u_input)\n",
    "x_pos_mean_common = common_set.get_dependent('x_pos_mean_common')['value']\n",
    "y_pos_mean_common = common_set.get_dependent('y_pos_mean_common')['value']\n",
    "z_pos_mean_common = common_set.get_dependent('z_pos_mean_common')['value']\n",
    "x_pos_std_common = common_set.get_dependent('x_pos_std_common')['value']\n",
    "y_pos_std_common = common_set.get_dependent('y_pos_std_common')['value']\n",
    "z_pos_std_common = common_set.get_dependent('z_pos_std_common')['value']\n",
    "\n",
    "# Experiment: Experimental Uncertainty (u_D)\n",
    "u_D_x = [statistics.mean([x, y]) for x, y in zip(x_pos_std_num_res, x_pos_std_common)]\n",
    "u_D_y = [statistics.mean([x, y]) for x, y in zip(y_pos_std_num_res, y_pos_std_common)]\n",
    "u_D_z = [statistics.mean([x, y]) for x, y in zip(z_pos_std_num_res, z_pos_std_common)]\n",
    "\n",
    "# Validation Uncertainty (u_val)\n",
    "u_val_x = np.sqrt(np.square(x_pos_std_num_res) + np.square(x_pos_std_common) + np.square(u_D_x))\n",
    "u_val_y = np.sqrt(np.square(y_pos_std_num_res) + np.square(y_pos_std_common) + np.square(u_D_y))\n",
    "u_val_z = np.sqrt(np.square(z_pos_std_num_res) + np.square(z_pos_std_common) + np.square(u_D_z))\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(time_common, x_pos_std_num_res, label='$u_{num}$')\n",
    "ax[1].plot(time_common, y_pos_std_num_res, label='$u_{num}$')\n",
    "ax[2].plot(time_common, z_pos_std_num_res, label='$u_{num}$')\n",
    "\n",
    "ax[0].plot(time_common, x_pos_std_common, label='$u_{input}$')\n",
    "ax[1].plot(time_common, y_pos_std_common, label='$u_{input}$')\n",
    "ax[2].plot(time_common, z_pos_std_common, label='$u_{input}$')\n",
    "\n",
    "ax[0].plot(time_common, u_D_x, label='$u_{D}$')\n",
    "ax[1].plot(time_common, u_D_y, label='$u_{D}$')\n",
    "ax[2].plot(time_common, u_D_z, label='$u_{D}$')\n",
    "\n",
    "ax[0].plot(time_common, u_val_x, label='$u_{val}$')\n",
    "ax[1].plot(time_common, u_val_y, label='$u_{val}$')\n",
    "ax[2].plot(time_common, u_val_z, label='$u_{val}$')\n",
    "\n",
    "ax[0].set_title(\"x uncertainties\")\n",
    "ax[1].set_title(\"y uncertainties\")\n",
    "ax[2].set_title(\"z uncertainties\")\n",
    "\n",
    "ax[0].set_xlabel(\"time\")\n",
    "ax[1].set_xlabel(\"time\")\n",
    "ax[2].set_xlabel(\"time\")\n",
    "\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[1].set_ylabel(\"Position\")\n",
    "ax[2].set_ylabel(\"Position\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_u_all.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Validation Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(time_common, x_pos_mean_common)\n",
    "ax[1].plot(time_common, y_pos_mean_common)\n",
    "ax[2].plot(time_common, z_pos_mean_common)\n",
    "\n",
    "ax[0].fill_between(time_common, x_pos_mean_common + 2 * u_val_x, x_pos_mean_common - 2 * u_val_x, alpha=0.25)\n",
    "ax[1].fill_between(time_common, y_pos_mean_common + 2 * u_val_y, y_pos_mean_common - 2 * u_val_y, alpha=0.25)\n",
    "ax[2].fill_between(time_common, z_pos_mean_common + 2 * u_val_z, z_pos_mean_common - 2 * u_val_z, alpha=0.25)\n",
    "\n",
    "ax[0].plot(time_true, x_true)\n",
    "ax[1].plot(time_true, y_true)\n",
    "ax[2].plot(time_true, z_true)\n",
    "\n",
    "ax[0].set_title(\"x_pos with u_val_x\")\n",
    "ax[1].set_title(\"y_pos with u_val_y\")\n",
    "ax[2].set_title(\"z_pos with u_val_z\")\n",
    "\n",
    "ax[0].set_xlabel(\"time\")\n",
    "ax[1].set_xlabel(\"time\")\n",
    "ax[2].set_xlabel(\"time\")\n",
    "\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[1].set_ylabel(\"Position\")\n",
    "ax[2].set_ylabel(\"Position\")\n",
    "\n",
    "ax[0].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[1].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[2].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_u_val.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification of Margins and Uncertainties (QMU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering Data\n",
    "\n",
    "# Data spanning 4 standard deviations\n",
    "x_list = np.linspace(x_pos_mean_common[-1] - 4 * u_val_x[-1], x_pos_mean_common[-1] + 4 * u_val_x[-1])\n",
    "y_list = np.linspace(y_pos_mean_common[-1] - 4 * u_val_y[-1], y_pos_mean_common[-1] + 4 * u_val_y[-1])\n",
    "z_list = np.linspace(z_pos_mean_common[-1] - 4 * u_val_z[-1], z_pos_mean_common[-1] + 4 * u_val_z[-1])\n",
    "\n",
    "# Normal Distribution\n",
    "x_dist = 1 / (np.sqrt(2 * np.pi * u_val_x[-1]**2)) * np.exp(- (x_list - x_pos_mean_common[-1])**2 / (2 * u_val_x[-1]**2))\n",
    "y_dist = 1 / (np.sqrt(2 * np.pi * u_val_y[-1]**2)) * np.exp(- (y_list - y_pos_mean_common[-1])**2 / (2 * u_val_y[-1]**2))\n",
    "z_dist = 1 / (np.sqrt(2 * np.pi * u_val_z[-1]**2)) * np.exp(- (z_list - z_pos_mean_common[-1])**2 / (2 * u_val_z[-1]**2))\n",
    "\n",
    "# Requirement\n",
    "Req_x = 75\n",
    "Req_y = 5\n",
    "Req_z = 80\n",
    "\n",
    "# Margin Factor\n",
    "MF_x = (Req_x - x_pos_mean_common[-1]) / u_val_x[-1]\n",
    "MF_y = (Req_y - y_pos_mean_common[-1]) / u_val_y[-1]\n",
    "MF_z = (Req_z - z_pos_mean_common[-1]) / u_val_z[-1]\n",
    "\n",
    "print(MF_x, MF_y, MF_z)\n",
    "\n",
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(x_list, x_dist, label='Normal PDF')\n",
    "ax[1].plot(y_list, y_dist, label='Normal PDF')\n",
    "ax[2].plot(z_list, z_dist, label='Normal PDF')\n",
    "\n",
    "ax[0].plot([x_pos_mean_common[-1], x_pos_mean_common[-1]], [0, max(x_dist)], label=f'$\\mu=${round(x_pos_mean_common[-1], 2)} with $\\sigma=${round(u_val_x[-1], 2)}')\n",
    "ax[1].plot([y_pos_mean_common[-1], y_pos_mean_common[-1]], [0, max(y_dist)], label=f'$\\mu=${round(y_pos_mean_common[-1], 2)} with $\\sigma=${round(u_val_y[-1], 2)}')\n",
    "ax[2].plot([z_pos_mean_common[-1], z_pos_mean_common[-1]], [0, max(z_dist)], label=f'$\\mu=${round(z_pos_mean_common[-1], 2)} with $\\sigma=${round(u_val_z[-1], 2)}')\n",
    "\n",
    "ax[0].plot([Req_x, Req_x], [0, max(x_dist)], label=f'$Req_x$ = {Req_x}')\n",
    "ax[1].plot([Req_y, Req_y], [0, max(y_dist)], label=f'$Req_y$ = {Req_y}')\n",
    "ax[2].plot([Req_z, Req_z], [0, max(z_dist)], label=f'$Req_z$ = {Req_z}')\n",
    "\n",
    "ax[0].plot([x_pos_mean_common[-1], Req_x], [max(x_dist), max(x_dist)], label=f'$MF_x$ = {round(MF_x, 2)}')\n",
    "ax[1].plot([y_pos_mean_common[-1], Req_y], [max(y_dist), max(y_dist)], label=f'$MF_y$ = {round(MF_y, 2)}')\n",
    "ax[2].plot([z_pos_mean_common[-1], Req_z], [max(z_dist), max(z_dist)], label=f'$MF_z$ = {round(MF_z, 2)}')\n",
    "\n",
    "ax[0].set_title(\"x_pos_final with u_val_x_final\")\n",
    "ax[1].set_title(\"y_pos_final with u_val_y_final\")\n",
    "ax[2].set_title(\"z_pos_final with u_val_z_final\")\n",
    "\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[2].set_xlabel(\"Position\")\n",
    "\n",
    "ax[0].legend(loc='lower left')\n",
    "ax[1].legend(loc='lower left')\n",
    "ax[2].legend(loc='lower left')\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_QMU.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cz-tutorials-demo",
   "language": "python",
   "name": "cz-tutorials-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
