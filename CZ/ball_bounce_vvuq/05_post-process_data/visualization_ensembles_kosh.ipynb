{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "**For more examples of what Kosh can do visit [GitHub Examples](https://github.com/LLNL/kosh/tree/stable/examples).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numbers import Number\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import kosh\n",
    "import math\n",
    "import statistics\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "if sys.argv[1] == '-f':  # Running as notebook\n",
    "    spec_root = ''\n",
    "    %matplotlib notebook\n",
    "else:\n",
    "    spec_root = sys.argv[1]  # Running as script\n",
    "\n",
    "# Ensembles Initialization\n",
    "database = os.path.join(spec_root, '../04_manage_data/data/ensembles_output.sqlite')\n",
    "target_type = \"csv_rec\"\n",
    "datastore = kosh.connect(database)\n",
    "print(\"Kosh is ready!\")\n",
    "\n",
    "# Baseline Initialization\n",
    "database_baseline = os.path.join(spec_root, '../01_baseline_simulation/baseline/data/baseline_output.sqlite')\n",
    "datastore_baseline = kosh.connect(database_baseline)\n",
    "group_id = '47bcda'\n",
    "val = list(datastore_baseline.find(id_pool = group_id + '_0'))[0]\n",
    "# Printing Attributes and Features\n",
    "print('Attributes:')\n",
    "print('\\t',val.list_attributes())\n",
    "print('\\n')\n",
    "print('Features Sets:')\n",
    "print('\\t',val.list_features())\n",
    "\n",
    "x_true = val['physics_cycle_series/x_pos'][:]\n",
    "y_true = val['physics_cycle_series/y_pos'][:]\n",
    "z_true = val['physics_cycle_series/z_pos'][:]\n",
    "time_true = val['physics_cycle_series/time'][:]\n",
    "\n",
    "# Numerical Resolution Initialization\n",
    "database_num_res = os.path.join(spec_root, '../01_baseline_simulation/num_res/data/num_res_output.sqlite')\n",
    "datastore_num_res = kosh.connect(database_num_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding Data to Records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    del_object = list(datastore.find(id = 'mean'))[0]\n",
    "    datastore.delete(del_object)\n",
    "except:\n",
    "    pass\n",
    "\n",
    "x_temp = np.array([])\n",
    "y_temp = np.array([])\n",
    "z_temp = np.array([])\n",
    "\n",
    "x_mean = np.array([])\n",
    "y_mean = np.array([])\n",
    "z_mean = np.array([])\n",
    "x_std = np.array([])\n",
    "y_std = np.array([])\n",
    "z_std = np.array([])\n",
    "\n",
    "total_recs = len(list(datastore.find(load_type='dictionary')))\n",
    "for i, dataset in enumerate(datastore.find(load_type='dictionary')): # Each record is now a dataset\n",
    "    \n",
    "    print(f'Record {i+1} of {total_recs}')\n",
    "    \n",
    "    x_pred = np.array(dataset['curve_sets']['physics_cycle_series']['dependent']['x_pos']['value'])\n",
    "    y_pred = np.array(dataset['curve_sets']['physics_cycle_series']['dependent']['y_pos']['value'])\n",
    "    z_pred = np.array(dataset['curve_sets']['physics_cycle_series']['dependent']['z_pos']['value'])\n",
    "\n",
    "    if x_temp.size==0:\n",
    "        x_temp = x_pred\n",
    "        y_temp = y_pred\n",
    "        z_temp = z_pred\n",
    "    else:      \n",
    "        x_temp = np.vstack((x_temp,x_pred))\n",
    "        y_temp = np.vstack((y_temp,y_pred))\n",
    "        z_temp = np.vstack((z_temp,z_pred))\n",
    "\n",
    "x_mean = np.mean(x_temp,axis=0)\n",
    "y_mean = np.mean(y_temp,axis=0)\n",
    "z_mean = np.mean(z_temp,axis=0)\n",
    "x_std = np.std(x_temp,axis=0)\n",
    "y_std = np.std(y_temp,axis=0)\n",
    "z_std = np.std(z_temp,axis=0)\n",
    "\n",
    "datastore.create(id = 'mean')\n",
    "mean = list(datastore.find(id_pool = 'mean'))[0]\n",
    "mean.add_curve(np.array(time_true).tolist(), \"mean_data\", 'time')\n",
    "mean.add_curve(x_mean.tolist(), \"mean_data\", 'x_pos_mean')\n",
    "mean.add_curve(y_mean.tolist(), \"mean_data\", 'y_pos_mean')\n",
    "mean.add_curve(z_mean.tolist(), \"mean_data\", 'z_pos_mean')\n",
    "mean.add_curve(x_std.tolist(), \"mean_data\", 'x_pos_std')\n",
    "mean.add_curve(y_std.tolist(), \"mean_data\", 'y_pos_std')\n",
    "mean.add_curve(z_std.tolist(), \"mean_data\", 'z_pos_std')\n",
    "\n",
    "mean.add_curve((x_mean + x_std).tolist(), \"mean_data\", 'x_pos_mean_plus_std')\n",
    "mean.add_curve((y_mean + y_std).tolist(), \"mean_data\", 'y_pos_mean_plus_std')\n",
    "mean.add_curve((z_mean + z_std).tolist(), \"mean_data\", 'z_pos_mean_plus_std')\n",
    "mean.add_curve((x_mean - x_std).tolist(), \"mean_data\", 'x_pos_mean_minus_std')\n",
    "mean.add_curve((y_mean - y_std).tolist(), \"mean_data\", 'y_pos_mean_minus_std')\n",
    "mean.add_curve((z_mean - z_std).tolist(), \"mean_data\", 'z_pos_mean_minus_std')\n",
    "\n",
    "mean.add_curve((x_mean + 2*x_std).tolist(), \"mean_data\", 'x_pos_mean_plus_2std')\n",
    "mean.add_curve((y_mean + 2*y_std).tolist(), \"mean_data\", 'y_pos_mean_plus_2std')\n",
    "mean.add_curve((z_mean + 2*z_std).tolist(), \"mean_data\", 'z_pos_mean_plus_2std')\n",
    "mean.add_curve((x_mean - 2*x_std).tolist(), \"mean_data\", 'x_pos_mean_minus_2std')\n",
    "mean.add_curve((y_mean - 2*y_std).tolist(), \"mean_data\", 'y_pos_mean_minus_2std')\n",
    "mean.add_curve((z_mean - 2*z_std).tolist(), \"mean_data\", 'z_pos_mean_minus_2std')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting Options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = plt.rcParams['axes.prop_cycle'].by_key()['color']\n",
    "scalars = [\"x_pos_final\", \"y_pos_final\", \"z_pos_final\"]\n",
    "parameters = ['x_pos_initial', 'y_pos_initial', 'z_pos_initial', 'x_vel_initial', 'y_vel_initial', 'z_vel_initial']\n",
    "convergence = [1, 2, 4, 8, 10, 16, 32, 64, 128, 256, 512, 1024]\n",
    "n_bins = int(math.sqrt(convergence[-1]))\n",
    "os.makedirs(os.path.join(spec_root, \"../05_post-process_data/images/\"), exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI transient data with uncertainty bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Gathering Data\n",
    "\n",
    "mean = list(datastore.find(id_pool = 'mean'))[0]\n",
    "\n",
    "time = mean['mean_data/time'][:]\n",
    "\n",
    "x_pos_mean = mean['mean_data/x_pos_mean'][:]\n",
    "y_pos_mean = mean['mean_data/y_pos_mean'][:]\n",
    "z_pos_mean = mean['mean_data/z_pos_mean'][:]\n",
    "\n",
    "x_pos_mean_plus_2std = mean['mean_data/x_pos_mean_plus_2std'][:]\n",
    "y_pos_mean_plus_2std = mean['mean_data/y_pos_mean_plus_2std'][:]\n",
    "z_pos_mean_plus_2std = mean['mean_data/z_pos_mean_plus_2std'][:]\n",
    "\n",
    "x_pos_mean_minus_2std = mean['mean_data/x_pos_mean_minus_2std'][:]\n",
    "y_pos_mean_minus_2std = mean['mean_data/y_pos_mean_minus_2std'][:]\n",
    "z_pos_mean_minus_2std = mean['mean_data/z_pos_mean_minus_2std'][:]\n",
    "\n",
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(time, x_pos_mean)\n",
    "ax[1].plot(time, y_pos_mean)\n",
    "ax[2].plot(time, z_pos_mean)\n",
    "\n",
    "ax[0].fill_between(time, x_pos_mean_plus_2std, x_pos_mean_minus_2std, alpha=0.25)\n",
    "ax[1].fill_between(time, y_pos_mean_plus_2std, y_pos_mean_minus_2std, alpha=0.25)\n",
    "ax[2].fill_between(time, z_pos_mean_plus_2std, z_pos_mean_minus_2std, alpha=0.25)\n",
    "\n",
    "ax[0].plot(time_true, x_true)\n",
    "ax[1].plot(time_true, y_true)\n",
    "ax[2].plot(time_true, z_true)\n",
    "\n",
    "ax[0].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[1].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[2].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "\n",
    "ax[0].set_xlabel('time')\n",
    "ax[0].set_ylabel('x_pos_mean')\n",
    "ax[0].set_title('x_pos_mean')\n",
    "\n",
    "ax[1].set_xlabel('time')\n",
    "ax[1].set_ylabel('y_pos_mean')\n",
    "ax[1].set_title('y_pos_mean')\n",
    "\n",
    "ax[2].set_xlabel('time')\n",
    "ax[2].set_ylabel('z_pos_mean')\n",
    "ax[2].set_title('z_pos_mean')\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_u_input.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data violin and box plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Gathering Data\n",
    "\n",
    "x_pos_final = [dataset['data']['x_pos_final']['value'] for dataset in datastore.find(load_type='dictionary') if dataset['id'] != 'mean']\n",
    "y_pos_final = [dataset['data']['y_pos_final']['value'] for dataset in datastore.find(load_type='dictionary') if dataset['id'] != 'mean']\n",
    "z_pos_final = [dataset['data']['z_pos_final']['value'] for dataset in datastore.find(load_type='dictionary') if dataset['id'] != 'mean']\n",
    "\n",
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].violinplot(x_pos_final)\n",
    "ax[0].boxplot(x_pos_final)\n",
    "ax[1].violinplot(y_pos_final)\n",
    "ax[1].boxplot(y_pos_final)\n",
    "ax[2].violinplot(z_pos_final)\n",
    "ax[2].boxplot(z_pos_final)\n",
    "\n",
    "ax[0].set_title(\"x_pos_final\")\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[0].set_xticklabels([\"x_pos_final\"])\n",
    "\n",
    "ax[1].set_title(\"y_pos_final\")\n",
    "ax[1].set_xticklabels([\"y_pos_final\"])\n",
    "\n",
    "ax[2].set_title(\"z_pos_final\")\n",
    "ax[2].set_xticklabels([\"z_pos_final\"])\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_violin_box.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data violin and box convergence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for scalar in scalars:\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 7))\n",
    "\n",
    "    for i, runs in enumerate(convergence):\n",
    "\n",
    "        convergence_ids = [group_id + \"_\" + str(x + 1) for x in range(runs)]  # We can do this because IDs have the run number\n",
    "\n",
    "        scalar_values = [dataset['data'][scalar]['value'] for dataset in datastore.find(id_pool = convergence_ids, load_type='dictionary')]\n",
    "\n",
    "        ax.violinplot(scalar_values, positions=[i])\n",
    "        ax.boxplot(scalar_values, positions=[i])\n",
    "\n",
    "    ax.set_title(scalar)\n",
    "    ax.set_xlabel(\"Simulations\")\n",
    "    ax.set_ylabel(\"Position\")\n",
    "    ax.set_xticklabels(convergence, rotation=45)\n",
    "\n",
    "    fig.savefig(os.path.join(spec_root, f\"../05_post-process_data/images/QoIs_{scalar}_violin_box_convergence.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data PDF and CDF plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "ax1 = []\n",
    "ax1.append(ax[0].twinx())\n",
    "ax1.append(ax[1].twinx())\n",
    "ax1.append(ax[2].twinx())\n",
    "\n",
    "# x\n",
    "ax[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, label='PDF')\n",
    "ax1[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, color=colors[1], label='CDF')\n",
    "\n",
    "ax[0].set_title(\"x_pos_final\")\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "lines, labels = ax[0].get_legend_handles_labels()\n",
    "lines2, labels2 = ax1[0].get_legend_handles_labels()\n",
    "ax[0].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# y\n",
    "ax[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True)\n",
    "ax1[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, color=colors[1])\n",
    "\n",
    "ax[1].set_title(\"y_pos_final\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[1].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "# z\n",
    "ax[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True)\n",
    "ax1[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, color=colors[1])\n",
    "\n",
    "ax[2].set_title(\"z_pos_final\")\n",
    "ax[2].set_xlabel(\"Position\")\n",
    "ax[2].legend(lines + lines2, labels + labels2, loc='upper left')\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_pdf_cdf.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data PDF and CDF convergence plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "fig1, ax1 = plt.subplots(nrows=1, ncols=3, figsize=(20, 5))\n",
    "\n",
    "for i, runs in enumerate(convergence):  # Depending on number of runs, can obscure the rest\n",
    "\n",
    "    convergence_ids = [group_id + \"_\" + str(x + 1) for x in range(runs)]  # We can do this because IDs have the run number\n",
    "\n",
    "    x_pos_final = [dataset['data']['x_pos_final']['value'] for dataset in datastore.find(id_pool = convergence_ids, load_type='dictionary')]\n",
    "    y_pos_final = [dataset['data']['y_pos_final']['value'] for dataset in datastore.find(id_pool = convergence_ids, load_type='dictionary')]\n",
    "    z_pos_final = [dataset['data']['z_pos_final']['value'] for dataset in datastore.find(id_pool = convergence_ids, load_type='dictionary')]\n",
    "\n",
    "    ax[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, label=runs)\n",
    "    ax[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True, label=runs)\n",
    "    ax[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True, label=runs)\n",
    "\n",
    "    ax1[0].hist(x_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, label=runs)\n",
    "    ax1[1].hist(y_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, label=runs)\n",
    "    ax1[2].hist(z_pos_final, bins=n_bins, histtype='step', density=True, cumulative=True, label=runs)\n",
    "\n",
    "# x\n",
    "ax[0].set_title(\"x_pos_final pdf\")\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "ax[0].legend()\n",
    "\n",
    "# y\n",
    "ax[1].set_title(\"y_pos_final pdf\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[1].legend()\n",
    "\n",
    "# z\n",
    "ax[2].set_title(\"z_pos_final pdf\")\n",
    "ax[2].set_xlabel(\"Position\")\n",
    "ax[2].legend()\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_pdf_convergence.png\"))\n",
    "\n",
    "# x\n",
    "ax1[0].set_title(\"x_pos_final cdf\")\n",
    "ax1[0].set_xlabel(\"Position\")\n",
    "ax1[0].legend()\n",
    "\n",
    "# y\n",
    "ax1[1].set_title(\"y_pos_final cdf\")\n",
    "ax1[1].set_xlabel(\"Position\")\n",
    "ax1[1].legend()\n",
    "\n",
    "# z\n",
    "ax1[2].set_title(\"z_pos_final cdf\")\n",
    "ax1[2].set_xlabel(\"Position\")\n",
    "ax1[2].legend()\n",
    "\n",
    "fig1.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_cdf_convergence.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data parameter correlation scatter plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_plts = len(parameters)\n",
    "rows_cols = math.ceil(math.sqrt(num_plts))\n",
    "\n",
    "all_scalars = scalars + parameters\n",
    "\n",
    "corrcoefs = {}  # Correlation Coefficient Dictionary that will be used in the next two cells\n",
    "\n",
    "for i, runs in enumerate(convergence):\n",
    "\n",
    "    convergence_ids = [group_id + \"_\" + str(x + 1) for x in range(runs)]  # We can do this because IDs have the run number\n",
    "\n",
    "    corrcoefs[runs] = {}\n",
    "\n",
    "    for scalar in scalars:\n",
    "\n",
    "        if runs == convergence[-1]:  # Just plot the last set of simulations\n",
    "\n",
    "            fig, ax = plt.subplots(nrows=rows_cols, ncols=rows_cols, figsize=(rows_cols * 5, rows_cols * 5))\n",
    "\n",
    "            fig.suptitle(scalar)\n",
    "\n",
    "            i = 0\n",
    "            j = 0\n",
    "            ax[j, i].set_ylabel(scalar)\n",
    "\n",
    "        scalar_values = [dataset['data'][scalar]['value'] for dataset in datastore.find(id_pool = convergence_ids, load_type='dictionary')]\n",
    "\n",
    "        corrcoefs[runs][scalar] = {}\n",
    "\n",
    "        for parameter in parameters:\n",
    "\n",
    "            parameter_values = [dataset['data'][parameter]['value'] for dataset in datastore.find(id_pool = convergence_ids, load_type='dictionary')]\n",
    "\n",
    "            r = np.corrcoef(parameter_values, scalar_values)[0, 1]\n",
    "            corrcoefs[runs][scalar][parameter] = r\n",
    "\n",
    "            if runs == convergence[-1]:  # Just plot the last set of simulations\n",
    "\n",
    "                m, b = np.polyfit(parameter_values, scalar_values, 1)\n",
    "                print(f\"m: {m}, r: {r}\")\n",
    "                x = np.linspace(min(parameter_values), max(parameter_values))\n",
    "                y = m * x + b\n",
    "\n",
    "#                 slope, intercept, r, p, se = stats.linregress(x, y)\n",
    "\n",
    "                ax[j, i].scatter(parameter_values, scalar_values)\n",
    "                ax[j, i].plot(x, y, color=colors[1], linewidth=2.0)\n",
    "                ax[j, i].set_title(f\"{parameter} r={round(r, 2)}\")\n",
    "\n",
    "                if i == rows_cols - 1:  # Cycling through subplots\n",
    "\n",
    "                    i = 0\n",
    "                    j += 1\n",
    "                    ax[j, i].set_ylabel(scalar)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    i += 1\n",
    "\n",
    "        if runs == convergence[-1]:  # Just plot the last set of simulations\n",
    "            fig.savefig(os.path.join(spec_root, f\"../05_post-process_data/images/QoIs_{scalar}_correlation.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data parameter correlation heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "cc_matrix = np.zeros((len(parameters), len(scalars)))\n",
    "\n",
    "for scalar in scalars:\n",
    "\n",
    "    for parameter in parameters:\n",
    "\n",
    "        cc_matrix[i, j] = corrcoefs[convergence[-1]][scalar][parameter]\n",
    "        i += 1\n",
    "\n",
    "    i = 0\n",
    "    j += 1\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "im = ax.imshow(cc_matrix)\n",
    "cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "ax.set_title(\"Correlation Heatmap\")\n",
    "\n",
    "ax.set_xlabel(\"QoI\")\n",
    "ax.set_xticks(np.arange(len(scalars)))\n",
    "ax.set_xticklabels(scalars, rotation=90, ha='center', minor=False)\n",
    "\n",
    "ax.set_ylabel(\"Parameter\")\n",
    "ax.set_yticks(np.arange(len(parameters)))\n",
    "ax.set_yticklabels(parameters, minor=False)\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, '../05_post-process_data/images/QoIs_correlation_heatmap'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QoI point data parameter correlation convergence heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "\n",
    "cc_matrix = np.zeros((len(parameters), len(convergence)))\n",
    "\n",
    "for scalar in scalars:\n",
    "\n",
    "    for runs in convergence:\n",
    "\n",
    "        for parameter in parameters:\n",
    "\n",
    "            cc_matrix[i, j] = corrcoefs[runs][scalar][parameter]\n",
    "            i += 1\n",
    "\n",
    "        i = 0\n",
    "        j += 1\n",
    "\n",
    "    i = 0\n",
    "    j = 0\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 10))\n",
    "    im = ax.imshow(cc_matrix)\n",
    "    cbar = ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "    ax.set_title(f\"Correlation Heatmap Convergence {scalar}\")\n",
    "\n",
    "    ax.set_xlabel(\"Simulations\")\n",
    "    ax.set_xticks(np.arange(len(convergence)))\n",
    "    ax.set_xticklabels(convergence, rotation=90, ha='center', minor=False)\n",
    "\n",
    "    ax.set_ylabel(\"Parameter\")\n",
    "    ax.set_yticks(np.arange(len(parameters)))\n",
    "    ax.set_yticklabels(parameters, minor=False)\n",
    "\n",
    "    fig.savefig(os.path.join(spec_root, f'../05_post-process_data/images/QoIs_{scalar}_correlation_heatmap_convergence'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adding up the uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding common timestep data to Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numerical Resolution\n",
    "# Be sure to run 01_baseline_simulation/num_res/visualization_num_res.ipynb to acquire necessary data\n",
    "mean_num_res = list(datastore_num_res.find(id_pool = 'mean'))[0]\n",
    "time_num_res = mean_num_res['mean_data/time_common'][:]\n",
    "\n",
    "# Ensembles\n",
    "mean = list(datastore.find(id_pool = 'mean'))[0]\n",
    "time = mean['mean_data/time'][:]\n",
    "x_pos_mean = mean['mean_data/x_pos_mean'][:]\n",
    "y_pos_mean = mean['mean_data/y_pos_mean'][:]\n",
    "z_pos_mean = mean['mean_data/z_pos_mean'][:]\n",
    "x_pos_std = mean['mean_data/x_pos_std'][:]\n",
    "y_pos_std = mean['mean_data/y_pos_std'][:]\n",
    "z_pos_std = mean['mean_data/z_pos_std'][:]\n",
    "\n",
    "x_pos_mean_common = []\n",
    "y_pos_mean_common = []\n",
    "z_pos_mean_common = []\n",
    "x_pos_std_common = []\n",
    "y_pos_std_common = []\n",
    "z_pos_std_common = []\n",
    "time_common = []\n",
    "\n",
    "for i, t in enumerate(time):\n",
    "\n",
    "    for t2 in time_num_res:\n",
    "\n",
    "        if t == t2:\n",
    "\n",
    "            x_pos_mean_common.append(x_pos_mean[i])\n",
    "            y_pos_mean_common.append(y_pos_mean[i])\n",
    "            z_pos_mean_common.append(z_pos_mean[i])\n",
    "            x_pos_std_common.append(x_pos_std[i])\n",
    "            y_pos_std_common.append(y_pos_std[i])\n",
    "            z_pos_std_common.append(z_pos_std[i])\n",
    "            time_common.append(time[i])\n",
    "\n",
    "            \n",
    "mean.add_curve(np.array(time_common).tolist(), \"common_data\", 'time_common')\n",
    "mean.add_curve(np.array(x_pos_mean_common).tolist(), \"common_data\", 'x_pos_mean_common')\n",
    "mean.add_curve(np.array(y_pos_mean_common).tolist(), \"common_data\", 'y_pos_mean_common')\n",
    "mean.add_curve(np.array(z_pos_mean_common).tolist(), \"common_data\", 'z_pos_mean_common')\n",
    "mean.add_curve(np.array(x_pos_std_common).tolist(), \"common_data\", 'x_pos_std_common')\n",
    "mean.add_curve(np.array(y_pos_std_common).tolist(), \"common_data\", 'y_pos_std_common')\n",
    "mean.add_curve(np.array(z_pos_std_common).tolist(), \"common_data\", 'z_pos_std_common')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Validation Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Numerical Resolution: Numerical Uncertainty (u_num)\n",
    "# Be sure to run 01_baseline_simulation/num_res/visualization_num_res.ipynb to acquire necessary data\n",
    "x_pos_std_num_res = mean_num_res['mean_data/x_pos_std'][:]\n",
    "y_pos_std_num_res = mean_num_res['mean_data/y_pos_std'][:]\n",
    "z_pos_std_num_res = mean_num_res['mean_data/z_pos_std'][:]\n",
    "\n",
    "# Ensembles: Input Uncertainty (u_input)\n",
    "x_pos_mean_common = mean['common_data/x_pos_mean_common'][:]\n",
    "y_pos_mean_common = mean['common_data/y_pos_mean_common'][:]\n",
    "z_pos_mean_common = mean['common_data/z_pos_mean_common'][:]\n",
    "x_pos_std_common = mean['common_data/x_pos_std_common'][:]\n",
    "y_pos_std_common = mean['common_data/y_pos_std_common'][:]\n",
    "z_pos_std_common = mean['common_data/z_pos_std_common'][:]\n",
    "\n",
    "# Experiment: Experimental Uncertainty (u_D)\n",
    "u_D_x = [statistics.mean([x, y]) for x, y in zip(x_pos_std_num_res, x_pos_std_common)]\n",
    "u_D_y = [statistics.mean([x, y]) for x, y in zip(y_pos_std_num_res, y_pos_std_common)]\n",
    "u_D_z = [statistics.mean([x, y]) for x, y in zip(z_pos_std_num_res, z_pos_std_common)]\n",
    "\n",
    "# Validation Uncertainty (u_val)\n",
    "u_val_x = np.sqrt(np.square(x_pos_std_num_res) + np.square(x_pos_std_common) + np.square(u_D_x))\n",
    "u_val_y = np.sqrt(np.square(y_pos_std_num_res) + np.square(y_pos_std_common) + np.square(u_D_y))\n",
    "u_val_z = np.sqrt(np.square(z_pos_std_num_res) + np.square(z_pos_std_common) + np.square(u_D_z))\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(time_common, x_pos_std_num_res, label='$u_{num}$')\n",
    "ax[1].plot(time_common, y_pos_std_num_res, label='$u_{num}$')\n",
    "ax[2].plot(time_common, z_pos_std_num_res, label='$u_{num}$')\n",
    "\n",
    "ax[0].plot(time_common, x_pos_std_common, label='$u_{input}$')\n",
    "ax[1].plot(time_common, y_pos_std_common, label='$u_{input}$')\n",
    "ax[2].plot(time_common, z_pos_std_common, label='$u_{input}$')\n",
    "\n",
    "ax[0].plot(time_common, u_D_x, label='$u_{D}$')\n",
    "ax[1].plot(time_common, u_D_y, label='$u_{D}$')\n",
    "ax[2].plot(time_common, u_D_z, label='$u_{D}$')\n",
    "\n",
    "ax[0].plot(time_common, u_val_x, label='$u_{val}$')\n",
    "ax[1].plot(time_common, u_val_y, label='$u_{val}$')\n",
    "ax[2].plot(time_common, u_val_z, label='$u_{val}$')\n",
    "\n",
    "ax[0].set_title(\"x uncertainties\")\n",
    "ax[1].set_title(\"y uncertainties\")\n",
    "ax[2].set_title(\"z uncertainties\")\n",
    "\n",
    "ax[0].set_xlabel(\"time\")\n",
    "ax[1].set_xlabel(\"time\")\n",
    "ax[2].set_xlabel(\"time\")\n",
    "\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[1].set_ylabel(\"Position\")\n",
    "ax[2].set_ylabel(\"Position\")\n",
    "\n",
    "ax[0].legend()\n",
    "ax[1].legend()\n",
    "ax[2].legend()\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_u_all.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting Validation Uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(time_common, x_pos_mean_common)\n",
    "ax[1].plot(time_common, y_pos_mean_common)\n",
    "ax[2].plot(time_common, z_pos_mean_common)\n",
    "\n",
    "ax[0].fill_between(time_common, x_pos_mean_common + 2 * u_val_x, x_pos_mean_common - 2 * u_val_x, alpha=0.25)\n",
    "ax[1].fill_between(time_common, y_pos_mean_common + 2 * u_val_y, y_pos_mean_common - 2 * u_val_y, alpha=0.25)\n",
    "ax[2].fill_between(time_common, z_pos_mean_common + 2 * u_val_z, z_pos_mean_common - 2 * u_val_z, alpha=0.25)\n",
    "\n",
    "ax[0].plot(time_true, x_true)\n",
    "ax[1].plot(time_true, y_true)\n",
    "ax[2].plot(time_true, z_true)\n",
    "\n",
    "ax[0].set_title(\"x_pos with u_val_x\")\n",
    "ax[1].set_title(\"y_pos with u_val_y\")\n",
    "ax[2].set_title(\"z_pos with u_val_z\")\n",
    "\n",
    "ax[0].set_xlabel(\"time\")\n",
    "ax[1].set_xlabel(\"time\")\n",
    "ax[2].set_xlabel(\"time\")\n",
    "\n",
    "ax[0].set_ylabel(\"Position\")\n",
    "ax[1].set_ylabel(\"Position\")\n",
    "ax[2].set_ylabel(\"Position\")\n",
    "\n",
    "ax[0].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[1].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "ax[2].legend(labels=['Simulation Mean', '$\\mu \\pm 2 \\sigma$', 'Validation Data'])\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_u_val.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quantification of Margins and Uncertainties (QMU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gathering Data\n",
    "\n",
    "# Data spanning 4 standard deviations\n",
    "x_list = np.linspace(x_pos_mean_common[-1] - 4 * u_val_x[-1], x_pos_mean_common[-1] + 4 * u_val_x[-1])\n",
    "y_list = np.linspace(y_pos_mean_common[-1] - 4 * u_val_y[-1], y_pos_mean_common[-1] + 4 * u_val_y[-1])\n",
    "z_list = np.linspace(z_pos_mean_common[-1] - 4 * u_val_z[-1], z_pos_mean_common[-1] + 4 * u_val_z[-1])\n",
    "\n",
    "# Normal Distribution\n",
    "x_dist = 1 / (np.sqrt(2 * np.pi * u_val_x[-1]**2)) * np.exp(- (x_list - x_pos_mean_common[-1])**2 / (2 * u_val_x[-1]**2))\n",
    "y_dist = 1 / (np.sqrt(2 * np.pi * u_val_y[-1]**2)) * np.exp(- (y_list - y_pos_mean_common[-1])**2 / (2 * u_val_y[-1]**2))\n",
    "z_dist = 1 / (np.sqrt(2 * np.pi * u_val_z[-1]**2)) * np.exp(- (z_list - z_pos_mean_common[-1])**2 / (2 * u_val_z[-1]**2))\n",
    "\n",
    "# Requirement\n",
    "Req_x = 75\n",
    "Req_y = 5\n",
    "Req_z = 80\n",
    "\n",
    "# Margin Factor\n",
    "MF_x = (Req_x - x_pos_mean_common[-1]) / u_val_x[-1]\n",
    "MF_y = (Req_y - y_pos_mean_common[-1]) / u_val_y[-1]\n",
    "MF_z = (Req_z - z_pos_mean_common[-1]) / u_val_z[-1]\n",
    "\n",
    "print(MF_x, MF_y, MF_z)\n",
    "\n",
    "# Plotting\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 5))\n",
    "\n",
    "ax[0].plot(x_list, x_dist, label='Normal PDF')\n",
    "ax[1].plot(y_list, y_dist, label='Normal PDF')\n",
    "ax[2].plot(z_list, z_dist, label='Normal PDF')\n",
    "\n",
    "ax[0].plot([x_pos_mean_common[-1], x_pos_mean_common[-1]], [0, max(x_dist)], label=f'$\\mu=${round(x_pos_mean_common[-1], 2)} with $\\sigma=${round(u_val_x[-1], 2)}')\n",
    "ax[1].plot([y_pos_mean_common[-1], y_pos_mean_common[-1]], [0, max(y_dist)], label=f'$\\mu=${round(y_pos_mean_common[-1], 2)} with $\\sigma=${round(u_val_y[-1], 2)}')\n",
    "ax[2].plot([z_pos_mean_common[-1], z_pos_mean_common[-1]], [0, max(z_dist)], label=f'$\\mu=${round(z_pos_mean_common[-1], 2)} with $\\sigma=${round(u_val_z[-1], 2)}')\n",
    "\n",
    "ax[0].plot([Req_x, Req_x], [0, max(x_dist)], label=f'$Req_x$ = {Req_x}')\n",
    "ax[1].plot([Req_y, Req_y], [0, max(y_dist)], label=f'$Req_y$ = {Req_y}')\n",
    "ax[2].plot([Req_z, Req_z], [0, max(z_dist)], label=f'$Req_z$ = {Req_z}')\n",
    "\n",
    "ax[0].plot([x_pos_mean_common[-1], Req_x], [max(x_dist), max(x_dist)], label=f'$MF_x$ = {round(MF_x, 2)}')\n",
    "ax[1].plot([y_pos_mean_common[-1], Req_y], [max(y_dist), max(y_dist)], label=f'$MF_y$ = {round(MF_y, 2)}')\n",
    "ax[2].plot([z_pos_mean_common[-1], Req_z], [max(z_dist), max(z_dist)], label=f'$MF_z$ = {round(MF_z, 2)}')\n",
    "\n",
    "ax[0].set_title(\"x_pos_final with u_val_x_final\")\n",
    "ax[1].set_title(\"y_pos_final with u_val_y_final\")\n",
    "ax[2].set_title(\"z_pos_final with u_val_z_final\")\n",
    "\n",
    "ax[0].set_xlabel(\"Position\")\n",
    "ax[1].set_xlabel(\"Position\")\n",
    "ax[2].set_xlabel(\"Position\")\n",
    "\n",
    "ax[0].legend(loc='lower left')\n",
    "ax[1].legend(loc='lower left')\n",
    "ax[2].legend(loc='lower left')\n",
    "\n",
    "fig.savefig(os.path.join(spec_root, \"../05_post-process_data/images/QoIs_QMU.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cz-tutorials-demo",
   "language": "python",
   "name": "cz-tutorials-demo"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
