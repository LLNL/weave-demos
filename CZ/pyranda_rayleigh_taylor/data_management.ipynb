{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2843b65e-c334-4859-86f4-022dfd54775b",
   "metadata": {},
   "source": [
    "# Data Management with Kosh\n",
    "\n",
    "After generating data with Maestro or Merlin we need to access the data in our Kosh/Sina store. After some data manipulation we can train a surrogate model to emulate the Pyranda physics calculations.\n",
    "\n",
    "The Kosh store gives us the convenience of saving all the variables and outputs with the associated metadata. Instead of saving varibales like the Atwood number and velocity-magnitude in the filenames we can save all the information we need in the metadata. Then it's convenient to find in our ensemble later. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f47f86-53f8-4482-82d1-bbaa3e63a9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import kosh\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "\n",
    "\n",
    "# Connect to the Kosh store\n",
    "store_dir = os.path.join(os.getcwd(), \"experiments/pyranda.sql\")\n",
    "store = kosh.connect(store_dir)\n",
    "\n",
    "# Create an ensemble or use an existing one\n",
    "# We can associate all our datasets and images with this ensemble\n",
    "name = os.listdir(os.path.join(os.getcwd(), \"RT_STUDIES\"))[0]\n",
    "try:\n",
    "    ensemble = next(store.find_ensembles(name=name))\n",
    "except Exception:\n",
    "    # Does not exist yet\n",
    "    ensemble = store.create_ensemble(name=name)\n",
    "\n",
    "print(f\"ensemble: {ensemble}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd243c76-b55a-4594-b7dd-6b647ae218fc",
   "metadata": {},
   "source": [
    "## Choosing times to evaluate mixing width\n",
    "\n",
    "We gathered a lot of data with the simulation runs. However, we need to choose specific points in time to evaluate mixing width. Here we can choose the number of time points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9567ae25-3d2a-452b-ab28-3ec6ab8adb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose number of times here\n",
    "NTpts = 1\n",
    "# All simulations run from 0.0 to at least 60.0 seconds\n",
    "Tmin = 0.0\n",
    "Tmax = 60.0\n",
    "\n",
    "# The time points are evenly spaced between 0.0 and 60.0\n",
    "sample_times = np.linspace(Tmin, Tmax, NTpts)\n",
    "# If we only want one point we will evaluate at the max time\n",
    "if len(sample_times) == 1:\n",
    "    sample_times = [Tmax]\n",
    "    \n",
    "print(f\"Model evaluation time/s {sample_times}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "467dcc0e-417e-415e-921a-5626c137ead2",
   "metadata": {},
   "source": [
    "## Gathering variables from each simulation run\n",
    "\n",
    "We can loop through the datasets in our Kosh store, and save the inputs and outputs of interest. Data of all different types can be associated together in our ensemble."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e015ab47-97a6-4d52-87c7-f82419833887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "samples = []\n",
    "N_cases = len(list(store.find(types=\"pyranda\", run_type=\"sim\", ids_only=True)))\n",
    "for i, case in enumerate(store.find(types=\"pyranda\", run_type=\"sim\"), start=1):\n",
    "    # Let's add this dataset to our ensemble\n",
    "    #print(\"*********************************\")\n",
    "    #print(\"DS:\", case.id)\n",
    "    #print(\"*********************************\")\n",
    "    ensemble.add(case)\n",
    "\n",
    "    # Let's retrieve the variables of interest\n",
    "    time = case[\"variables/time\"][:] # Time\n",
    "    width = case[\"variables/mixing width\"][:] # Width\n",
    "    mixed = case[\"variables/mixedness\"][:] # Mixedness\n",
    "    atwood = case.atwood_number\n",
    "    velocity = case.velocity_magnitude\n",
    "    lbl = f\"Vel: {velocity} - At: {atwood}\"\n",
    "    plt.figure(2)\n",
    "    plt.plot(time, width, \"-o\", label=lbl)\n",
    "    for st in sample_times:\n",
    "        plt.axvline(x=st, color='b', label=f\"{st} s\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Mix Width\")\n",
    "    plt.title(\"Rayleigh-Taylor Simulations\")\n",
    "    if i == N_cases:\n",
    "        fnm = \"all_mixing_width.png\"\n",
    "        ensemble.associate(fnm, \"png\", metadata={\"title\": lbl})\n",
    "\n",
    "    # Plotting to show the input sampling design\n",
    "    plt.figure(1)\n",
    "    plt.plot(atwood, velocity, 'ko')\n",
    "    plt.xlabel(\"Atwood number\")\n",
    "    plt.ylabel(\"Velocity magnitude\")\n",
    "    plt.title(\"Latin Hypercube Space-Filling Design\")\n",
    "    if i == N_cases:\n",
    "        fnm = \"atwood_vs_vel.png\"\n",
    "        ensemble.associate(fnm, \"png\", metadata={\"title\":'atwood vs velocity'})\n",
    "            \n",
    "    # For each time, qoi, get NTpts\n",
    "    #  Sample = [atwood, velocity, w(0), w(1), w(2) ...]\n",
    "    sample_widths = np.interp(sample_times, time, width)\n",
    "    sample = np.insert( sample_widths, 0, atwood)\n",
    "    sample = np.insert( sample, 1, velocity)\n",
    "    samples.append( sample )\n",
    "\n",
    "plt.show()\n",
    "        \n",
    "samples = np.array(samples)\n",
    "\n",
    "# Save for next step\n",
    "header = f\"# 'atwood' 'velocity' \"\n",
    "for ii in range(NTpts):\n",
    "    header += \" 'width-%s' \" % ii\n",
    "fnm = f\"rt_sim_data.csv\"\n",
    "np.savetxt(fnm, samples, delimiter=',',header=header)\n",
    "#associate with ensemble\n",
    "ensemble.associate(fnm, \"pandas/csv\", metadata={\"gp_data\":True})\n",
    "\n",
    "print(f\"Data size: {samples.shape}\")\n",
    "print(\"First 5 rows\")\n",
    "print(samples[:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d8510f-56c5-4acf-b81f-20ee4360c6a8",
   "metadata": {},
   "source": [
    "## Fitting the Gaussian Process (GP) Models\n",
    "\n",
    "We will fit a Gaussian process surrogate model for each time we chose from the simulation. This model will need to be able to predict mixing width very quickly and accurately. The Gaussian process model can return a prediction and a standard error estimate. The error should be very small for data points it was trained on, and larger when it has to interpolate between training data points. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6184617d-4c8a-4774-9274-9b18341f0f01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need an array of model inputs for the GP\n",
    "xgp = samples[:,0:2]\n",
    "# The GP model performs better when the inputs are scaled\n",
    "scaler = MMS()\n",
    "scaled_samples = scaler.fit_transform(xgp)\n",
    "\n",
    "# Get inputs for 2D plots\n",
    "# We're going to evaluate the model at points it was not trained on\n",
    "atwoods    = np.linspace(.25,.75, 100)\n",
    "velocities = np.linspace(.75, 1.25, 100)\n",
    "at2d, vel2d = np.meshgrid(atwoods, velocities)\n",
    "atwoods = at2d.flatten().reshape(-1,1)\n",
    "velocities = vel2d.flatten().reshape(-1,1)\n",
    "inputs = np.concatenate( (atwoods, velocities), axis=1 )\n",
    "scaled_inputs = scaler.transform(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c356c9d-2c4d-4987-a077-c789db5d982a",
   "metadata": {},
   "source": [
    "## Plotting the GP Predictions\n",
    "\n",
    "Here we're going to evaluate the model at many inputs to see how well it can predict mixing width. The mix width prediction is shown as a blue response surface over the Atwood and velocity values. The color of the surface indicates low error blue to high error red. The error is very low in the areas where the model was trained, but we forced to model to predict outside of the range it was trained on and you can see the error increases at the edges where it did not have traning data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9580ee0f-5990-45c3-bf28-dba0dd5fadf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "GP_times = []\n",
    "\n",
    "# Fitting a GP model for NTpts in time\n",
    "for ii in range(NTpts):\n",
    "    sample_time = sample_times[ii]\n",
    "    y = samples[:, 2 + ii]  # Get width at this time-slice\n",
    "    GP_times.append(GPR().fit(scaled_samples, y))\n",
    "\n",
    "    # See GP prediction in 2D\n",
    "    pred, std = GP_times[ii].predict(scaled_inputs, return_std=True)\n",
    "\n",
    "    fig_num = 3 + ii\n",
    "    fig = plt.figure(fig_num)\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    pred2d = pred.reshape(at2d.shape)\n",
    "    std2d = std.reshape(at2d.shape)\n",
    "    mycol = cm.jet( 1.5*(std2d-std.min())/(std.max()-std.min())    )\n",
    "    ax.plot_surface(at2d, vel2d, pred2d,facecolors=mycol)\n",
    "    ax.set_xlabel('Atwood')\n",
    "    ax.set_ylabel('Velocity')\n",
    "    ax.set_zlabel('Width')\n",
    "    fnm = f\"GP_at_{sample_time}_s.png\"\n",
    "    ensemble.associate(fnm, \"png\", metadata={\"title\":'2D GP'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bdb8a4d-804d-4ebd-b818-7120f86ce04b",
   "metadata": {},
   "source": [
    "## Leave-One-Out Cross Validation\n",
    "\n",
    "We can evaluate the GP model with a leave-one-out method where a model is trained on all the data except the first data point, next we train a model on all but the second data point, etc. Each time we train a model we evaluate it on the data point that was left out. It's ok to use in this situation because our model is fast to train and predict. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f8af5-35e0-4eda-814a-f9f3541745a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "loo = LeaveOneOut()\n",
    "\n",
    "outputs = samples[:, 2:]\n",
    "\n",
    "time_point_error = []\n",
    "for ii in range(NTpts):\n",
    "    loo_pred = []\n",
    "    loo_bar = []\n",
    "    loo_sqerror = []\n",
    "    for i, (train_index, test_index) in enumerate(loo.split(scaled_samples)):\n",
    "        y = outputs[:, ii]\n",
    "        gp_model = GPR().fit(scaled_samples[train_index, :], y[train_index])\n",
    "        pred, std = gp_model.predict(scaled_samples[test_index, :], return_std=True)\n",
    "        loo_pred.append(pred)\n",
    "        loo_bar.append((pred + std * 1.96) - (pred - std * 1.96))\n",
    "        loo_sqerror.append((y[test_index] - pred)**2)\n",
    "    plt.figure(3 + NTpts + ii)\n",
    "    plt.errorbar(outputs[:, ii].flatten(), np.array(loo_pred).flatten(), yerr=np.array(loo_bar).flatten(), fmt='o',label='GP')\n",
    "    plt.plot([2,7],[2,7],'r-',label=\"Exact\")\n",
    "    plt.xlabel(\"Actual Mix Width\")\n",
    "    plt.ylabel(\"Predicted Mix Width\")\n",
    "    plt.title(f\"GP Model at {sample_times[ii]} s\")\n",
    "    plt.legend()\n",
    "    print(f\"MSE at {sample_times[ii]} s: {sum(loo_sqerror)/len(loo_sqerror)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_tik_env",
   "language": "python",
   "name": "weave_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
