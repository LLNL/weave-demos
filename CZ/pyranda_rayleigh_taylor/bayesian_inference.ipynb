{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06908b17-e8b0-4873-942c-475ec27135a1",
   "metadata": {},
   "source": [
    "# Bayesian Inference with IBIS's MCMC Sampling\n",
    "\n",
    "In order to better understand our model inputs, Atwood number and velocity, we're going to use Bayesian inference to combine our model data and prior knowledge with observed, experimental data. In this case the posterior distribution that we're after is the distribution of values of mixing width at a certain time, and that distribution has been informed by both the physics model and observed data. Since it can be complicated to compute the posterior distribution we will use IBIS's MCMC sampling module to approximate it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587c94b2-4af6-4c9d-94c4-b8d2f79a7bbf",
   "metadata": {},
   "source": [
    "## Connect to the Kosh store to access datasets\n",
    "\n",
    "Again, we can connect to our Kosh store and ensemble to conveniently have access to all the data we need. We're going to need both the simulation data and experimental data for MCMC sampling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f0ec54-a0dd-4694-86d3-239058303ee3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import kosh\n",
    "import scipy.stats as sts\n",
    "from sklearn.preprocessing import MinMaxScaler as MMS\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor as GPR\n",
    "from ibis import mcmc\n",
    "import os\n",
    "import matplotlib\n",
    "matplotlib.use('agg')\n",
    "\n",
    "\n",
    "# Connect to the Kosh store and read in datasets\n",
    "store_dir = os.path.join(os.getcwd(), \"Experiments/pyranda.sql\")\n",
    "store = kosh.connect(store_dir)\n",
    "\n",
    "# The experimental data was saved to the ensemble in our Kosh store\n",
    "experiments_ensemble = next(store.find_ensembles(name=\"experiments\"))\n",
    "exp_uri = next(experiments_ensemble.find(mime_type=\"pandas/csv\")).uri\n",
    "\n",
    "# The simulation data was just generated with the workflow manager (Maestro or Merlin)\n",
    "name = os.listdir(os.path.join(os.getcwd(), \"RT_STUDIES\"))[0]\n",
    "sim_ensemble = next(store.find_ensembles(name=name))\n",
    "sim_uri = next(sim_ensemble.find(mime_type=\"pandas/csv\")).uri\n",
    "\n",
    "# Use the URI to read in datasets\n",
    "rt_exp_data = np.genfromtxt(exp_uri, delimiter=',')\n",
    "rt_sim_data = np.genfromtxt(sim_uri, delimiter=',')\n",
    "\n",
    "# Separate inputs and outputs for experimental and simulation data\n",
    "xexp = rt_exp_data[:,:2]\n",
    "yexp = rt_exp_data[:,2:]\n",
    "xsim = rt_sim_data[:,:2]\n",
    "ysim = rt_sim_data[:,2:]\n",
    "\n",
    "# How many models do we have to evaluate?\n",
    "# Find the number of outputs\n",
    "nmodels = ysim.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48ff45a7-822f-4382-a54d-5061fc3f4ce8",
   "metadata": {},
   "source": [
    "## Train the Surrogate Model\n",
    "\n",
    "Let's train our Gaussian process model on the simulation data. We want this surrogate model to be able to emulate the real simulation, but provide predictions much faster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30aea8a3-754f-4522-9847-509d03cb322e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The GP model performs better when the inputs are scaled\n",
    "scaler = MMS()\n",
    "scaled_samples = scaler.fit_transform(xsim)\n",
    "\n",
    "# Build the GP surrogates for the number of models\n",
    "surrogates = {}\n",
    "for i in range(nmodels):\n",
    "    ygp = ysim[:,i]\n",
    "    name = output_names[i]\n",
    "    surrogates[name] = GPR().fit(scaled_samples, ygp)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df579985-19fe-4149-98a4-1ebdb8d769aa",
   "metadata": {},
   "source": [
    "## Defining variables for MCMC sampling\n",
    "\n",
    "We need to provide a lot of information for the MCMC sampling function. We'll start by defining information for the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4958d207-38a1-453c-bfd1-4612f6987aad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Start by instantiating the mcmc object\n",
    "default_mcmc = mcmc.DefaultMCMC()\n",
    "\n",
    "# Input information\n",
    "input_names = ['atwood_num', 'vel_mag']\n",
    "\n",
    "# Calculate standard deviation for simulation input features\n",
    "sim_std = np.std(xsim, axis=1)\n",
    "\n",
    "# We define the min and max for each input\n",
    "ranges = [[.3, .8], [.7, 1.3] ]\n",
    "\n",
    "# Add inputs\n",
    "# We're using uninformative priors for both inputs\n",
    "for i, name in enumerate(input_names):\n",
    "    default_mcmc.add_input(name, 0.0, 1.0, sim_std[i], sts.uniform().pdf,\n",
    "                           unscaled_low=ranges[i][0], unscaled_high=ranges[i][1], scaling='lin')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8185c038-7405-4c4e-915f-5cb5e4bb91db",
   "metadata": {},
   "source": [
    "Next we define information for the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "988132fa-736a-474b-89b7-0b1fca1422cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output information\n",
    "output_names = []\n",
    "for i in range(nmodels):\n",
    "    output_names.append( 'mix_width-%s' % i)\n",
    "\n",
    "# Calculate experimental mean and standard deviation\n",
    "# At this time we can only provide one experimental value\n",
    "expMean = yexp.mean(axis=0)\n",
    "expStd  = yexp.std(axis=0)\n",
    "\n",
    "# Add outputs\n",
    "for name, mean, std in zip(output_names, expMean, expStd):\n",
    "    if \"10\" in name:\n",
    "        default_mcmc.add_output(\"RTexp\", name, surrogates[name], mean , std, input_names )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445c5292-e660-4bde-9fae-6daad4cb9f03",
   "metadata": {},
   "source": [
    "## Run MCMC Sampling\n",
    "\n",
    "We will define the rest of the information and run the chains for sampling the posterior distribution.\n",
    "\n",
    "total: We are going to run chains of \"total\" in length. That's the total number of samples, but we will need to check the trace plot to see how the sampling looks. \n",
    "\n",
    "Start: Start means a starting value for each input. Here we are starting with .5 for both Atwood number and velocity. These are not likely to be the values that product the highest probability for the posterior distribution. So the sampling may explore different local maximums until it finds the global maximum. \n",
    "\n",
    "burn: We will want to only keep the samples that are exploring around the global maximum. The inital samples may wander, and we will drop them from the samples we keep. This is called the \"burn-in\" period. In this case we will drop the first 500 samples.\n",
    "\n",
    "every: There is some correlation between the samples in the chain. The closer the samples are together, the more correlated they are. If we keep every 2 samples in this case, it reduces the correlation between our samples.\n",
    "\n",
    "n_chains: We can run multiple independent sampling chains in parallel.\n",
    "\n",
    "prior_only: Whether to run the chains on just the prior distributions.\n",
    "\n",
    "seed: The random seed for the Metropolis-Hastings algorithm that chooses the next sample for the chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fedee-4379-4e47-997e-476844536dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the MCMC chains to get samples approximating the posterior distribution\n",
    "default_mcmc.run_chain(total=10000,\n",
    "                       burn=500,\n",
    "                       every=2,\n",
    "                       start={name: .5 for name in input_names},\n",
    "                       n_chains=3,\n",
    "                       prior_only=False,\n",
    "                       seed=15)\n",
    "\n",
    "chains = default_mcmc.get_chains(flattened=False, scaled=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7f38c7-cb43-4c64-b6b3-0cd1f5863f93",
   "metadata": {},
   "source": [
    "## MCMC Sampling Plots\n",
    "\n",
    "We'll start by looking at the trace plots. We want to see a good sampling of the space, and since we already removed the \"burn-in\" samples the center of the mixing should stay in one place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e24216-604d-4904-a3b7-508be7a5179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_n in input_names:\n",
    "    plot = default_mcmc.trace_plot(input_name=input_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4220672-0768-4945-9276-c5998b73667f",
   "metadata": {},
   "source": [
    "Next we will view the histogram showing us a rough approximation of the posterior distribution for the mixing width. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfb47a5-0e85-47e3-a3b3-050152545050",
   "metadata": {},
   "outputs": [],
   "source": [
    "outvar = list(default_mcmc.outputs.keys())[0]\n",
    "post_pp = default_mcmc.posterior_predictive_plot(outvar, bins=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7cdcb89-1b2e-4782-9374-d84fe108b76d",
   "metadata": {},
   "source": [
    "Now we can see the more informed distributions for our uncertain parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db05384f-a0b7-4f6f-8463-dcbf03e7d2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_n in input_names:\n",
    "    hist_plot = default_mcmc.histogram_plot(input_name=input_n, bins=25, density=True, alpha=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac86b5-db9d-497d-a793-178da0c41f05",
   "metadata": {},
   "source": [
    "In the future we could use these distributions for Atwood number and density to make better predictions with error estimates."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_tik_env",
   "language": "python",
   "name": "weave_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
